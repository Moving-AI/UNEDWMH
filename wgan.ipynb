{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25228,
     "status": "ok",
     "timestamp": 1558116562593,
     "user": {
      "displayName": "Gonzalo Izaguirre de Diego",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCpLYqbfMklo9sOPH3YrDMvbujoQ9DA9F42bYLdXg=s64",
      "userId": "16828157033214543773"
     },
     "user_tz": -120
    },
    "id": "5vCJONb-lLgx",
    "outputId": "4f931f49-99ef-494f-c59c-924030f9588d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 553,
     "status": "ok",
     "timestamp": 1558116609541,
     "user": {
      "displayName": "Gonzalo Izaguirre de Diego",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCpLYqbfMklo9sOPH3YrDMvbujoQ9DA9F42bYLdXg=s64",
      "userId": "16828157033214543773"
     },
     "user_tz": -120
    },
    "id": "67g850TRlpn3",
    "outputId": "7aa3b45d-a633-4b24-ce53-aaf0acc53008"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/app\n",
      "['__pycache__', 'output', 'Weights', 'total_three_datasets_sorted.npy', 'funciones_wgan.py', 'wgan.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()\n",
    "#path = os.join(os.getcwd(), '/drive/app')\n",
    "\n",
    "path = r'/content/drive/My Drive/app'\n",
    "os.chdir(path)\n",
    "print(os.getcwd())\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2276,
     "status": "ok",
     "timestamp": 1558088225011,
     "user": {
      "displayName": "Javier Gamazo Tejero",
      "photoUrl": "",
      "userId": "04168543225743776047"
     },
     "user_tz": -120
    },
    "id": "DwxpkQ1Gs515",
    "outputId": "9e6441b3-92cb-4f65-b9d1-261403be7412"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import funciones_wgan as f\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, Conv2DTranspose\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZdIOCyyDkwXJ"
   },
   "source": [
    "# Variables generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NS5xllRdkwXK"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 9000\n",
    "BATCH_SIZE = 8\n",
    "# The training ratio is the number of discriminator updates\n",
    "# per generator update. The paper uses 5.\n",
    "TRAINING_RATIO = 1\n",
    "GRADIENT_PENALTY_WEIGHT = 10  # As per the paper\n",
    "INPUT_LEN = 128\n",
    "output_dir = 'output/'\n",
    "discriminator_weights = 'Weights/discriminator_epoch_950.h5'\n",
    "generator_weights = 'Weights/generator_epoch_950.h5'\n",
    "muestra = True # Si queremos coger una muestra de las imágenes. En true se cogen 140 imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ceeoa4I2QETJ"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  initial_epoch = int(generator_weights.split('_')[2].split('.')[0]) + 1\n",
    "except:\n",
    "  initial_epoch = 0\n",
    "final_epoch = initial_epoch + EPOCHS - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9DP95mVWs52C"
   },
   "source": [
    "# Discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F_h5Fxwes52D"
   },
   "outputs": [],
   "source": [
    "def make_discriminator():\n",
    "    \"\"\"Creates a discriminator model that takes an image as input and outputs a single\n",
    "    value, representing whether the input is real or generated. Unlike normal GANs, the\n",
    "    output is not sigmoid and does not represent a probability! Instead, the output\n",
    "    should be as large and negative as possible for generated inputs and as large and\n",
    "    positive as possible for real inputs.\n",
    "    Note that the improved WGAN paper suggests that BatchNormalization should not be\n",
    "    used in the discriminator.\"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 5, padding='same', strides=[2, 2], input_shape=(256, 256, 3)))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Convolution2D(64, 5, kernel_initializer='he_normal', strides=[2, 2], padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Convolution2D(128, 5, kernel_initializer='he_normal', padding='same', strides=[2, 2]))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Convolution2D(256, 5, kernel_initializer='he_normal', padding='same', strides=[2, 2]))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Convolution2D(512, 5, kernel_initializer='he_normal', padding='same', strides=[2, 2]))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Convolution2D(1024, 5, kernel_initializer='he_normal', padding='same', strides=[2, 2]))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(1024 * 4 * 4, kernel_initializer='he_normal'))\n",
    "    #model.add(LeakyReLU())\n",
    "    model.add(Dense(1, kernel_initializer='he_normal'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tT79oszus52F"
   },
   "source": [
    "# Generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhzjPvqXs52G"
   },
   "outputs": [],
   "source": [
    "def make_generator():\n",
    "    \"\"\"Creates a generator model that takes a 128-dimensional noise vector as a \"seed\",\n",
    "    and outputs images of size 256x256x3.\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(4 * 4 * 2048, input_dim=INPUT_LEN))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Reshape((4, 4, 2048), input_shape=(4 * 4 * 2048,)))\n",
    "    bn_axis = -1\n",
    "\n",
    "    model.add(Conv2DTranspose(1024, 5, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization(axis=bn_axis))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Conv2DTranspose(512, 5, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization(axis=bn_axis))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Conv2DTranspose(256, 5, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization(axis=bn_axis))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Conv2DTranspose(128, 5, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization(axis=bn_axis))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Conv2DTranspose(64, 5, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization(axis=bn_axis))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Conv2DTranspose(3, 5, strides=2, padding='same', activation='tanh'))\n",
    "    # El output de esta última es 256x256x3\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yO9tAxixs52I"
   },
   "source": [
    "# Carga de datos\n",
    "\n",
    "El archivo total_three_datasets_sorted.npy es una matriz que ya tiene las tres capas:\n",
    "* 0: T1\n",
    "* 1: FLAIR\n",
    "* 2: Máscara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lAk6-T1Ms52J"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    images = np.load('total_three_datasets_sorted.npy')\n",
    "except:\n",
    "    images = np.load('images_three_datasets_sorted.npy')\n",
    "    masks = np.load('masks_three_datasets_sorted.npy')\n",
    "    \n",
    "    # Normalizado entre -1 y +1. Esto lo hace sobre toda la imagen, no sobre la capa T1 o FLAIR por separado.\n",
    "    # No tengo muy claro que sea correcto\n",
    "    images = [2.*(image - np.min(image))/np.ptp(image) - 1 for image in images]\n",
    "        \n",
    "    images = np.concatenate((images, masks), axis=3)\n",
    "    \n",
    "    # El generador toma imágenes 256x256x3. Como las tenemos 200x200, hay que redimensionarlas:\n",
    "    dim_final = (256, 256)\n",
    "    images = np.array([cv2.resize(image, dim_final, interpolation = cv2.INTER_AREA) for image in images])\n",
    "    \n",
    "    np.save('total_three_datasets_sorted.npy', images)\n",
    "    del masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15195,
     "status": "ok",
     "timestamp": 1558088238021,
     "user": {
      "displayName": "Javier Gamazo Tejero",
      "photoUrl": "",
      "userId": "04168543225743776047"
     },
     "user_tz": -120
    },
    "id": "g-3cz_OKs52L",
    "outputId": "e086e696-dddd-40d1-c509-7ce3fcc34a66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2780, 256, 256, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O4QMHxqis52P"
   },
   "source": [
    "Si se trabaja con Tensorflow está bien porque los canales están en la última dimensión del array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9Idd_Htx7vU"
   },
   "outputs": [],
   "source": [
    "if muestra:\n",
    "  images_shuff = images[:]\n",
    "  np.random.shuffle(images_shuff)\n",
    "  images_shuff = images_shuff[0:140,...]\n",
    "  images = images_shuff[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16091,
     "status": "ok",
     "timestamp": 1558088238937,
     "user": {
      "displayName": "Javier Gamazo Tejero",
      "photoUrl": "",
      "userId": "04168543225743776047"
     },
     "user_tz": -120
    },
    "id": "WxFWfW7xs52P",
    "outputId": "efa6e737-7fd3-4970-e746-c5dec6700929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9656766 -1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.amin(images[34,...,0]), np.amin(images[34,...,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16085,
     "status": "ok",
     "timestamp": 1558088238938,
     "user": {
      "displayName": "Javier Gamazo Tejero",
      "photoUrl": "",
      "userId": "04168543225743776047"
     },
     "user_tz": -120
    },
    "id": "HgLnLY-wkwXc",
    "outputId": "504613ee-e272-4fd7-93ca-e2a4262aa746"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    }
   ],
   "source": [
    "n_images = images.shape[0]\n",
    "print(n_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NfpmgyW4kwXe"
   },
   "source": [
    "# Redes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ywbKy8DdkwXf"
   },
   "source": [
    "Inicialización de generador y discriminador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17751,
     "status": "ok",
     "timestamp": 1558088240613,
     "user": {
      "displayName": "Javier Gamazo Tejero",
      "photoUrl": "",
      "userId": "04168543225743776047"
     },
     "user_tz": -120
    },
    "id": "s-WR3_Xas52c",
    "outputId": "bc5f5159-f6bf-4319-afe8-5a927239b271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "generator = make_generator()\n",
    "discriminator = make_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wpMFotL6kwXk"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    generator.load_weights(generator_weights)\n",
    "    discriminator.load_weights(discriminator_weights)\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2DrfbXc-kwXm"
   },
   "source": [
    "### Generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GTOoa2ens52e"
   },
   "outputs": [],
   "source": [
    "# The generator_model is used when we want to train the generator layers.\n",
    "# As such, we ensure that the discriminator layers are not trainable.\n",
    "# Note that once we compile this model, updating .trainable will have no effect within\n",
    "# it. As such, it won't cause problems if we later set discriminator.trainable = True\n",
    "# for the discriminator_model, as long as we compile the generator_model first.\n",
    "for layer in discriminator.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "discriminator.trainable = False\n",
    "generator_input = Input(shape=(INPUT_LEN,))\n",
    "generator_layers = generator(generator_input)\n",
    "discriminator_layers_for_generator = discriminator(generator_layers)\n",
    "generator_model = Model(inputs=[generator_input], outputs=[discriminator_layers_for_generator])\n",
    "# We use the Adam paramaters from Gulrajani et al.\n",
    "generator_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9), loss=f.wasserstein_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMHdQ35WkwXr"
   },
   "source": [
    "### Discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "exsb00Ffs52g"
   },
   "outputs": [],
   "source": [
    "# Now that the generator_model is compiled, we can make the discriminator\n",
    "# layers trainable.\n",
    "for layer in discriminator.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "for layer in generator.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "discriminator.trainable = True\n",
    "generator.trainable = False\n",
    "\n",
    "# The discriminator_model is more complex. It takes both real image samples and random\n",
    "# noise seeds as input. The noise seed is run through the generator model to get\n",
    "# generated images. Both real and generated images are then run through the\n",
    "# discriminator. Although we could concatenate the real and generated images into a\n",
    "# single tensor, we don't (see model compilation for why).\n",
    "real_samples = Input(shape=images.shape[1:])\n",
    "generator_input_for_discriminator = Input(shape=(INPUT_LEN,))\n",
    "generated_samples_for_discriminator = generator(generator_input_for_discriminator)\n",
    "discriminator_output_from_generator = discriminator(generated_samples_for_discriminator)\n",
    "discriminator_output_from_real_samples = discriminator(real_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zSFtg_Qhs52k"
   },
   "outputs": [],
   "source": [
    "# We also need to generate weighted-averages of real and generated samples,\n",
    "# to use for the gradient norm penalty.\n",
    "averaged_samples = f.RandomWeightedAverage()([real_samples,\n",
    "                                            generated_samples_for_discriminator])\n",
    "# We then run these samples through the discriminator as well. Note that we never\n",
    "# really use the discriminator output for these samples - we're only running them to\n",
    "# get the gradient norm for the gradient penalty loss.\n",
    "averaged_samples_out = discriminator(averaged_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4TmGyJyhs52m"
   },
   "outputs": [],
   "source": [
    "# The gradient penalty loss function requires the input averaged samples to get\n",
    "# gradients. However, Keras loss functions can only have two arguments, y_true and\n",
    "# y_pred. We get around this by making a partial() of the function with the averaged\n",
    "# samples here.\n",
    "partial_gp_loss = f.partial(f.gradient_penalty_loss,\n",
    "                            averaged_samples=averaged_samples, gradient_penalty_weight=GRADIENT_PENALTY_WEIGHT)\n",
    "# Functions need names or Keras will throw an error\n",
    "partial_gp_loss.__name__ = 'gradient_penalty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8xqzj98js52p"
   },
   "outputs": [],
   "source": [
    "# Keras requires that inputs and outputs have the same number of samples. This is why\n",
    "# we didn't concatenate the real samples and generated samples before passing them to\n",
    "# the discriminator: If we had, it would create an output with 2 * BATCH_SIZE samples,\n",
    "# while the output of the \"averaged\" samples for gradient penalty\n",
    "# would have only BATCH_SIZE samples.\n",
    "\n",
    "# If we don't concatenate the real and generated samples, however, we get three\n",
    "# outputs: One of the generated samples, one of the real samples, and one of the\n",
    "# averaged samples, all of size BATCH_SIZE. This works neatly!\n",
    "discriminator_model = Model(inputs=[real_samples, generator_input_for_discriminator],\n",
    "                            outputs=[discriminator_output_from_real_samples, discriminator_output_from_generator, averaged_samples_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rgkg3-r4s52r"
   },
   "outputs": [],
   "source": [
    "# We use the Adam paramaters from Gulrajani et al. We use the Wasserstein loss for both\n",
    "# the real and generated samples, and the gradient penalty loss for the averaged samples\n",
    "discriminator_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),\n",
    "                            loss=[f.wasserstein_loss, f.wasserstein_loss, partial_gp_loss])\n",
    "\n",
    "# We make three label vectors for training. positive_y is the label vector for real\n",
    "# samples, with value 1. negative_y is the label vector for generated samples, with\n",
    "# value -1. The dummy_y vector is passed to the gradient_penalty loss function and\n",
    "# is not used.\n",
    "positive_y = np.ones((BATCH_SIZE, 1), dtype=np.float32)\n",
    "negative_y = -positive_y\n",
    "dummy_y = np.zeros((BATCH_SIZE, 1), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85462
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2330867,
     "status": "error",
     "timestamp": 1558098111416,
     "user": {
      "displayName": "Javier Gamazo Tejero",
      "photoUrl": "",
      "userId": "04168543225743776047"
     },
     "user_tz": -120
    },
    "id": "9c7EK-V-s52t",
    "outputId": "2b120ce0-6c9e-468f-b468-7dde90727b96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  951\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 951 took 18.042806386947632\n",
      "Epoch:  952\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 952 took 7.583513975143433\n",
      "Epoch:  953\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 953 took 7.593877077102661\n",
      "Epoch:  954\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 954 took 7.6227052211761475\n",
      "Epoch:  955\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 955 took 7.643261671066284\n",
      "Epoch:  956\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 956 took 7.661677122116089\n",
      "Epoch:  957\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 957 took 7.682616949081421\n",
      "Epoch:  958\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 958 took 7.694772720336914\n",
      "Epoch:  959\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 959 took 7.704302787780762\n",
      "Epoch:  960\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 960 took 7.713277816772461\n",
      "Epoch:  961\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 961 took 7.727752923965454\n",
      "Epoch:  962\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 962 took 7.734522342681885\n",
      "Epoch:  963\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 963 took 7.740330696105957\n",
      "Epoch:  964\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 964 took 7.7739646434783936\n",
      "Epoch:  965\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 965 took 7.773517847061157\n",
      "Epoch:  966\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 966 took 7.786100387573242\n",
      "Epoch:  967\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 967 took 7.814149618148804\n",
      "Epoch:  968\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 968 took 7.800434350967407\n",
      "Epoch:  969\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 969 took 7.825706720352173\n",
      "Epoch:  970\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 970 took 7.822282075881958\n",
      "Epoch:  971\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 971 took 7.822754383087158\n",
      "Epoch:  972\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 972 took 7.838008165359497\n",
      "Epoch:  973\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 973 took 7.842214822769165\n",
      "Epoch:  974\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 974 took 7.851227283477783\n",
      "Epoch:  975\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 975 took 7.862684011459351\n",
      "Epoch:  976\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 976 took 7.863980054855347\n",
      "Epoch:  977\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 977 took 7.860248565673828\n",
      "Epoch:  978\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 978 took 7.858649730682373\n",
      "Epoch:  979\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 979 took 7.8574488162994385\n",
      "Epoch:  980\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 980 took 7.855790615081787\n",
      "Epoch:  981\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 981 took 7.8630125522613525\n",
      "Epoch:  982\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 982 took 7.851561069488525\n",
      "Epoch:  983\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 983 took 7.864142894744873\n",
      "Epoch:  984\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 984 took 7.860728025436401\n",
      "Epoch:  985\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 985 took 7.86260724067688\n",
      "Epoch:  986\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 986 took 7.850820541381836\n",
      "Epoch:  987\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 987 took 7.855712175369263\n",
      "Epoch:  988\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 988 took 7.8665382862091064\n",
      "Epoch:  989\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 989 took 7.859070777893066\n",
      "Epoch:  990\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 990 took 7.864513397216797\n",
      "Epoch:  991\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 991 took 7.848743200302124\n",
      "Epoch:  992\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 992 took 7.861692667007446\n",
      "Epoch:  993\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 993 took 7.838144063949585\n",
      "Epoch:  994\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 994 took 7.84922456741333\n",
      "Epoch:  995\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 995 took 7.862715721130371\n",
      "Epoch:  996\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 996 took 7.853901147842407\n",
      "Epoch:  997\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 997 took 7.8620991706848145\n",
      "Epoch:  998\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 998 took 7.867749214172363\n",
      "Epoch:  999\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 999 took 7.8542420864105225\n",
      "Epoch:  1000\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1000 took 7.855752468109131\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_1000.png\n",
      "Epoch:  1001\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1001 took 7.880674839019775\n",
      "Epoch:  1002\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1002 took 7.850234270095825\n",
      "Epoch:  1003\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1003 took 7.880741596221924\n",
      "Epoch:  1004\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1004 took 7.894001483917236\n",
      "Epoch:  1005\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1005 took 7.872398614883423\n",
      "Epoch:  1006\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1006 took 7.870689868927002\n",
      "Epoch:  1007\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1007 took 7.850810766220093\n",
      "Epoch:  1008\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1008 took 7.843005895614624\n",
      "Epoch:  1009\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1009 took 7.825448513031006\n",
      "Epoch:  1010\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1010 took 7.849241256713867\n",
      "Epoch:  1011\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1011 took 7.840754270553589\n",
      "Epoch:  1012\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1012 took 7.848136901855469\n",
      "Epoch:  1013\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1013 took 7.849639654159546\n",
      "Epoch:  1014\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1014 took 7.851653814315796\n",
      "Epoch:  1015\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1015 took 7.853475570678711\n",
      "Epoch:  1016\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1016 took 7.852307558059692\n",
      "Epoch:  1017\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1017 took 7.851515293121338\n",
      "Epoch:  1018\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1018 took 7.848074913024902\n",
      "Epoch:  1019\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1019 took 7.855081081390381\n",
      "Epoch:  1020\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1020 took 7.854250907897949\n",
      "Epoch:  1021\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1021 took 7.870605707168579\n",
      "Epoch:  1022\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1022 took 7.863706588745117\n",
      "Epoch:  1023\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1023 took 7.845287561416626\n",
      "Epoch:  1024\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1024 took 7.83411979675293\n",
      "Epoch:  1025\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1025 took 7.845402240753174\n",
      "Epoch:  1026\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1026 took 7.841603994369507\n",
      "Epoch:  1027\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1027 took 7.859603404998779\n",
      "Epoch:  1028\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1028 took 7.842066049575806\n",
      "Epoch:  1029\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1029 took 7.839141130447388\n",
      "Epoch:  1030\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1030 took 7.851063251495361\n",
      "Epoch:  1031\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1031 took 7.837686777114868\n",
      "Epoch:  1032\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1032 took 7.8391149044036865\n",
      "Epoch:  1033\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1033 took 7.857732534408569\n",
      "Epoch:  1034\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1034 took 7.856112480163574\n",
      "Epoch:  1035\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1035 took 7.855236530303955\n",
      "Epoch:  1036\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1036 took 7.834937810897827\n",
      "Epoch:  1037\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1037 took 7.844576597213745\n",
      "Epoch:  1038\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1038 took 7.851511716842651\n",
      "Epoch:  1039\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1039 took 7.848608732223511\n",
      "Epoch:  1040\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1040 took 7.847414016723633\n",
      "Epoch:  1041\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1041 took 7.840516805648804\n",
      "Epoch:  1042\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1042 took 7.84235692024231\n",
      "Epoch:  1043\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1043 took 7.856967449188232\n",
      "Epoch:  1044\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1044 took 7.853783369064331\n",
      "Epoch:  1045\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1045 took 7.853976011276245\n",
      "Epoch:  1046\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1046 took 7.847027063369751\n",
      "Epoch:  1047\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1047 took 7.854307174682617\n",
      "Epoch:  1048\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1048 took 7.859036445617676\n",
      "Epoch:  1049\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1049 took 7.847012758255005\n",
      "Epoch:  1050\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1050 took 7.855422496795654\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_1050.png\n",
      "Epoch:  1051\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1051 took 7.892087697982788\n",
      "Epoch:  1052\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1052 took 7.84026312828064\n",
      "Epoch:  1053\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1053 took 7.85093355178833\n",
      "Epoch:  1054\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1054 took 7.863141775131226\n",
      "Epoch:  1055\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1055 took 7.859182834625244\n",
      "Epoch:  1056\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1056 took 7.862652063369751\n",
      "Epoch:  1057\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1057 took 7.85293436050415\n",
      "Epoch:  1058\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1058 took 7.8453264236450195\n",
      "Epoch:  1059\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1059 took 7.848820209503174\n",
      "Epoch:  1060\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1060 took 7.850318908691406\n",
      "Epoch:  1061\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1061 took 7.8317530155181885\n",
      "Epoch:  1062\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1062 took 7.83116340637207\n",
      "Epoch:  1063\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1063 took 7.835945129394531\n",
      "Epoch:  1064\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1064 took 7.85045599937439\n",
      "Epoch:  1065\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1065 took 7.851136207580566\n",
      "Epoch:  1066\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1066 took 7.8424718379974365\n",
      "Epoch:  1067\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1067 took 7.848204612731934\n",
      "Epoch:  1068\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1068 took 7.8540198802948\n",
      "Epoch:  1069\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1069 took 7.8518126010894775\n",
      "Epoch:  1070\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1070 took 7.862957954406738\n",
      "Epoch:  1071\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1071 took 7.847248315811157\n",
      "Epoch:  1072\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1072 took 7.848907947540283\n",
      "Epoch:  1073\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1073 took 7.87792444229126\n",
      "Epoch:  1074\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1074 took 7.843062400817871\n",
      "Epoch:  1075\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1075 took 7.837273359298706\n",
      "Epoch:  1076\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1076 took 7.845365047454834\n",
      "Epoch:  1077\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1077 took 7.845505952835083\n",
      "Epoch:  1078\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1078 took 7.8584747314453125\n",
      "Epoch:  1079\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1079 took 7.850514650344849\n",
      "Epoch:  1080\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1080 took 7.84184193611145\n",
      "Epoch:  1081\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1081 took 7.8572046756744385\n",
      "Epoch:  1082\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1082 took 7.847932577133179\n",
      "Epoch:  1083\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1083 took 7.840950965881348\n",
      "Epoch:  1084\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1084 took 7.83643364906311\n",
      "Epoch:  1085\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1085 took 7.861068964004517\n",
      "Epoch:  1086\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1086 took 7.848555564880371\n",
      "Epoch:  1087\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1087 took 7.84291410446167\n",
      "Epoch:  1088\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1088 took 7.852945327758789\n",
      "Epoch:  1089\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1089 took 7.84705114364624\n",
      "Epoch:  1090\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1090 took 7.850493669509888\n",
      "Epoch:  1091\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1091 took 7.846433639526367\n",
      "Epoch:  1092\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1092 took 7.838760852813721\n",
      "Epoch:  1093\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1093 took 7.854538440704346\n",
      "Epoch:  1094\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1094 took 7.844076871871948\n",
      "Epoch:  1095\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1095 took 7.856009483337402\n",
      "Epoch:  1096\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1096 took 7.846215009689331\n",
      "Epoch:  1097\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1097 took 7.847999811172485\n",
      "Epoch:  1098\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1098 took 7.853752613067627\n",
      "Epoch:  1099\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1099 took 7.86479377746582\n",
      "Epoch:  1100\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1100 took 7.856750726699829\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_1100.png\n",
      "Epoch:  1101\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1101 took 7.878485679626465\n",
      "Epoch:  1102\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1102 took 7.853915452957153\n",
      "Epoch:  1103\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1103 took 7.88265323638916\n",
      "Epoch:  1104\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1104 took 7.8788347244262695\n",
      "Epoch:  1105\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1105 took 7.861395359039307\n",
      "Epoch:  1106\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1106 took 7.879297971725464\n",
      "Epoch:  1107\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1107 took 7.870119094848633\n",
      "Epoch:  1108\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1108 took 7.856884479522705\n",
      "Epoch:  1109\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1109 took 7.836337089538574\n",
      "Epoch:  1110\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1110 took 7.831075191497803\n",
      "Epoch:  1111\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1111 took 7.840776205062866\n",
      "Epoch:  1112\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1112 took 7.829903602600098\n",
      "Epoch:  1113\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1113 took 7.8277318477630615\n",
      "Epoch:  1114\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1114 took 7.839160919189453\n",
      "Epoch:  1115\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1115 took 7.84831976890564\n",
      "Epoch:  1116\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1116 took 7.84853982925415\n",
      "Epoch:  1117\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1117 took 7.867159843444824\n",
      "Epoch:  1118\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1118 took 7.860773801803589\n",
      "Epoch:  1119\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1119 took 7.854564905166626\n",
      "Epoch:  1120\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1120 took 7.857752561569214\n",
      "Epoch:  1121\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1121 took 7.8526084423065186\n",
      "Epoch:  1122\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1122 took 7.854936838150024\n",
      "Epoch:  1123\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1123 took 7.847812652587891\n",
      "Epoch:  1124\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1124 took 7.850849390029907\n",
      "Epoch:  1125\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1125 took 7.849497079849243\n",
      "Epoch:  1126\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1126 took 7.83918833732605\n",
      "Epoch:  1127\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1127 took 7.855862617492676\n",
      "Epoch:  1128\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1128 took 7.860268592834473\n",
      "Epoch:  1129\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1129 took 7.849350452423096\n",
      "Epoch:  1130\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1130 took 7.855206489562988\n",
      "Epoch:  1131\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1131 took 7.867987871170044\n",
      "Epoch:  1132\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1132 took 7.856256723403931\n",
      "Epoch:  1133\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1133 took 7.847183465957642\n",
      "Epoch:  1134\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1134 took 7.833489179611206\n",
      "Epoch:  1135\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1135 took 7.848843812942505\n",
      "Epoch:  1136\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1136 took 7.846604585647583\n",
      "Epoch:  1137\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1137 took 7.854014873504639\n",
      "Epoch:  1138\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1138 took 7.86043119430542\n",
      "Epoch:  1139\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1139 took 7.8543701171875\n",
      "Epoch:  1140\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1140 took 7.82870888710022\n",
      "Epoch:  1141\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1141 took 7.836072206497192\n",
      "Epoch:  1142\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1142 took 7.845476150512695\n",
      "Epoch:  1143\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1143 took 7.845593214035034\n",
      "Epoch:  1144\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1144 took 7.841135025024414\n",
      "Epoch:  1145\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1145 took 7.849699020385742\n",
      "Epoch:  1146\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1146 took 7.849152326583862\n",
      "Epoch:  1147\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1147 took 7.8414106369018555\n",
      "Epoch:  1148\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1148 took 7.849409818649292\n",
      "Epoch:  1149\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1149 took 7.8694164752960205\n",
      "Epoch:  1150\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1150 took 7.842211008071899\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_1150.png\n",
      "Epoch:  1151\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1151 took 7.895936012268066\n",
      "Epoch:  1152\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1152 took 7.8526082038879395\n",
      "Epoch:  1153\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1153 took 7.873638391494751\n",
      "Epoch:  1154\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1154 took 7.875072479248047\n",
      "Epoch:  1155\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1155 took 7.865954160690308\n",
      "Epoch:  1156\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1156 took 7.853856325149536\n",
      "Epoch:  1157\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1157 took 7.855562448501587\n",
      "Epoch:  1158\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1158 took 7.84924578666687\n",
      "Epoch:  1159\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1159 took 7.836907386779785\n",
      "Epoch:  1160\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1160 took 7.830755233764648\n",
      "Epoch:  1161\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1161 took 7.845280647277832\n",
      "Epoch:  1162\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1162 took 7.847205877304077\n",
      "Epoch:  1163\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1163 took 7.845031261444092\n",
      "Epoch:  1164\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1164 took 7.859831809997559\n",
      "Epoch:  1165\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1165 took 7.849846601486206\n",
      "Epoch:  1166\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1166 took 7.849055290222168\n",
      "Epoch:  1167\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1167 took 7.861485242843628\n",
      "Epoch:  1168\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1168 took 7.856990337371826\n",
      "Epoch:  1169\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1169 took 7.862030267715454\n",
      "Epoch:  1170\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1170 took 7.865330934524536\n",
      "Epoch:  1171\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1171 took 7.847585678100586\n",
      "Epoch:  1172\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1172 took 7.828430414199829\n",
      "Epoch:  1173\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1173 took 7.850322008132935\n",
      "Epoch:  1174\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1174 took 7.842196226119995\n",
      "Epoch:  1175\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1175 took 7.8429036140441895\n",
      "Epoch:  1176\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1176 took 7.8319079875946045\n",
      "Epoch:  1177\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1177 took 7.8459765911102295\n",
      "Epoch:  1178\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1178 took 7.839464902877808\n",
      "Epoch:  1179\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1179 took 7.83321213722229\n",
      "Epoch:  1180\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1180 took 7.830888986587524\n",
      "Epoch:  1181\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1181 took 7.852379560470581\n",
      "Epoch:  1182\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1182 took 7.831889867782593\n",
      "Epoch:  1183\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1183 took 7.8444952964782715\n",
      "Epoch:  1184\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1184 took 7.839454889297485\n",
      "Epoch:  1185\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1185 took 7.839371919631958\n",
      "Epoch:  1186\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1186 took 7.829057931900024\n",
      "Epoch:  1187\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1187 took 7.850262880325317\n",
      "Epoch:  1188\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1188 took 7.85183572769165\n",
      "Epoch:  1189\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1189 took 7.8469555377960205\n",
      "Epoch:  1190\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1190 took 7.843825340270996\n",
      "Epoch:  1191\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1191 took 7.841928958892822\n",
      "Epoch:  1192\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1192 took 7.850160360336304\n",
      "Epoch:  1193\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1193 took 7.837371587753296\n",
      "Epoch:  1194\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1194 took 7.84686279296875\n",
      "Epoch:  1195\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1195 took 7.85973858833313\n",
      "Epoch:  1196\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1196 took 7.853793144226074\n",
      "Epoch:  1197\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1197 took 7.8433802127838135\n",
      "Epoch:  1198\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1198 took 7.857964277267456\n",
      "Epoch:  1199\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1199 took 7.853072881698608\n",
      "Epoch:  1200\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1200 took 7.841189622879028\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_1200.png\n",
      "Epoch:  1201\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1201 took 7.891613006591797\n",
      "Epoch:  1202\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1202 took 7.849449396133423\n",
      "Epoch:  1203\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1203 took 7.871263027191162\n",
      "Epoch:  1204\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1204 took 7.883575916290283\n",
      "Epoch:  1205\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1205 took 7.862382411956787\n",
      "Epoch:  1206\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1206 took 7.862256288528442\n",
      "Epoch:  1207\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1207 took 7.851793527603149\n",
      "Epoch:  1208\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1208 took 7.860341310501099\n",
      "Epoch:  1209\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1209 took 7.849727153778076\n",
      "Epoch:  1210\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1210 took 7.837433576583862\n",
      "Epoch:  1211\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1211 took 7.830679893493652\n",
      "Epoch:  1212\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1212 took 7.854002475738525\n",
      "Epoch:  1213\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1213 took 7.8577821254730225\n",
      "Epoch:  1214\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1214 took 7.847798824310303\n",
      "Epoch:  1215\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1215 took 7.870144844055176\n",
      "Epoch:  1216\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1216 took 7.8657591342926025\n",
      "Epoch:  1217\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1217 took 7.838977098464966\n",
      "Epoch:  1218\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1218 took 7.864671468734741\n",
      "Epoch:  1219\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1219 took 7.850985527038574\n",
      "Epoch:  1220\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1220 took 7.850879669189453\n",
      "Epoch:  1221\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1221 took 7.843142032623291\n",
      "Epoch:  1222\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1222 took 7.84614634513855\n",
      "Epoch:  1223\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1223 took 7.8531880378723145\n",
      "Epoch:  1224\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1224 took 7.852538108825684\n",
      "Epoch:  1225\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1225 took 7.8479907512664795\n",
      "Epoch:  1226\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1226 took 7.854074001312256\n",
      "Epoch:  1227\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1227 took 7.846136093139648\n",
      "Epoch:  1228\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1228 took 7.841863632202148\n",
      "Epoch:  1229\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1229 took 7.8667237758636475\n",
      "Epoch:  1230\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1230 took 7.853653907775879\n",
      "Epoch:  1231\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1231 took 7.857032299041748\n",
      "Epoch:  1232\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1232 took 7.854421615600586\n",
      "Epoch:  1233\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1233 took 7.8499345779418945\n",
      "Epoch:  1234\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1234 took 7.8434529304504395\n",
      "Epoch:  1235\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1235 took 7.840280771255493\n",
      "Epoch:  1236\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1236 took 7.84533429145813\n",
      "Epoch:  1237\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1237 took 7.848487854003906\n",
      "Epoch:  1238\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1238 took 7.844621419906616\n",
      "Epoch:  1239\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1239 took 7.8703436851501465\n",
      "Epoch:  1240\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1240 took 7.848511457443237\n",
      "Epoch:  1241\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1241 took 7.856848955154419\n",
      "Epoch:  1242\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1242 took 7.851112127304077\n",
      "Epoch:  1243\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1243 took 7.856984853744507\n",
      "Epoch:  1244\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1244 took 7.8457536697387695\n",
      "Epoch:  1245\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1245 took 7.8421266078948975\n",
      "Epoch:  1246\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1246 took 7.857389688491821\n",
      "Epoch:  1247\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1247 took 7.839602947235107\n",
      "Epoch:  1248\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1248 took 7.853512287139893\n",
      "Epoch:  1249\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1249 took 7.856999158859253\n",
      "Epoch:  1250\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1250 took 7.85467004776001\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_1250.png\n",
      "Epoch:  1251\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1251 took 7.8917951583862305\n",
      "Epoch:  1252\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1252 took 7.840613603591919\n",
      "Epoch:  1253\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1253 took 7.864678859710693\n",
      "Epoch:  1254\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1254 took 7.875464200973511\n",
      "Epoch:  1255\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1255 took 7.861342430114746\n",
      "Epoch:  1256\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1256 took 7.855154752731323\n",
      "Epoch:  1257\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1257 took 7.849194765090942\n",
      "Epoch:  1258\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1258 took 7.84473180770874\n",
      "Epoch:  1259\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1259 took 7.85030198097229\n",
      "Epoch:  1260\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1260 took 7.83122444152832\n",
      "Epoch:  1261\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1261 took 7.837830066680908\n",
      "Epoch:  1262\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1262 took 7.845066070556641\n",
      "Epoch:  1263\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1263 took 7.836575984954834\n",
      "Epoch:  1264\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1264 took 7.848568916320801\n",
      "Epoch:  1265\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1265 took 7.857349157333374\n",
      "Epoch:  1266\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1266 took 7.85284423828125\n",
      "Epoch:  1267\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1267 took 7.834697484970093\n",
      "Epoch:  1268\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1268 took 7.852901220321655\n",
      "Epoch:  1269\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1269 took 7.871232986450195\n",
      "Epoch:  1270\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1270 took 7.838738918304443\n",
      "Epoch:  1271\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1271 took 7.844749212265015\n",
      "Epoch:  1272\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1272 took 7.851417064666748\n",
      "Epoch:  1273\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1273 took 7.850451469421387\n",
      "Epoch:  1274\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1274 took 7.851560354232788\n",
      "Epoch:  1275\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1275 took 7.8434531688690186\n",
      "Epoch:  1276\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1276 took 7.8415422439575195\n",
      "Epoch:  1277\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1277 took 7.850485801696777\n",
      "Epoch:  1278\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1278 took 7.857100248336792\n",
      "Epoch:  1279\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1279 took 7.862924575805664\n",
      "Epoch:  1280\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1280 took 7.849104642868042\n",
      "Epoch:  1281\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1281 took 7.857780694961548\n",
      "Epoch:  1282\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1282 took 7.843745946884155\n",
      "Epoch:  1283\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1283 took 7.832132577896118\n",
      "Epoch:  1284\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1284 took 7.840416193008423\n",
      "Epoch:  1285\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1285 took 7.867831468582153\n",
      "Epoch:  1286\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1286 took 7.85712194442749\n",
      "Epoch:  1287\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1287 took 7.855306386947632\n",
      "Epoch:  1288\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1288 took 7.848632335662842\n",
      "Epoch:  1289\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1289 took 7.854243040084839\n",
      "Epoch:  1290\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1290 took 7.865183591842651\n",
      "Epoch:  1291\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1291 took 7.8504798412323\n",
      "Epoch:  1292\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1292 took 7.852945566177368\n",
      "Epoch:  1293\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1293 took 7.862909317016602\n",
      "Epoch:  1294\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1294 took 7.83665919303894\n",
      "Epoch:  1295\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1295 took 7.851024627685547\n",
      "Epoch:  1296\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1296 took 7.84457540512085\n",
      "Epoch:  1297\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1297 took 7.82655930519104\n",
      "Epoch:  1298\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1298 took 7.84890341758728\n",
      "Epoch:  1299\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1299 took 7.844741582870483\n",
      "Epoch:  1300\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1300 took 7.844764709472656\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_1300.png\n",
      "Epoch:  1301\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1301 took 7.862226486206055\n",
      "Epoch:  1302\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1302 took 7.849051237106323\n",
      "Epoch:  1303\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1303 took 7.875564098358154\n",
      "Epoch:  1304\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1304 took 7.9021759033203125\n",
      "Epoch:  1305\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1305 took 7.8715760707855225\n",
      "Epoch:  1306\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1306 took 7.859339714050293\n",
      "Epoch:  1307\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1307 took 7.846115350723267\n",
      "Epoch:  1308\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1308 took 7.836573600769043\n",
      "Epoch:  1309\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1309 took 7.855270624160767\n",
      "Epoch:  1310\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1310 took 7.837825536727905\n",
      "Epoch:  1311\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1311 took 7.8375842571258545\n",
      "Epoch:  1312\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1312 took 7.850560665130615\n",
      "Epoch:  1313\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1313 took 7.847476005554199\n",
      "Epoch:  1314\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1314 took 7.857801914215088\n",
      "Epoch:  1315\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1315 took 7.854967832565308\n",
      "Epoch:  1316\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1316 took 7.867919683456421\n",
      "Epoch:  1317\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1317 took 7.850716590881348\n",
      "Epoch:  1318\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1318 took 7.861278057098389\n",
      "Epoch:  1319\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1319 took 7.860662937164307\n",
      "Epoch:  1320\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1320 took 7.84345555305481\n",
      "Epoch:  1321\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1321 took 7.848789930343628\n",
      "Epoch:  1322\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1322 took 7.845739841461182\n",
      "Epoch:  1323\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1323 took 7.860271453857422\n",
      "Epoch:  1324\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1324 took 7.846658229827881\n",
      "Epoch:  1325\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1325 took 7.851583957672119\n",
      "Epoch:  1326\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1326 took 7.855006217956543\n",
      "Epoch:  1327\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1327 took 7.859863996505737\n",
      "Epoch:  1328\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1328 took 7.845627307891846\n",
      "Epoch:  1329\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1329 took 7.862172842025757\n",
      "Epoch:  1330\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1330 took 7.853200912475586\n",
      "Epoch:  1331\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1331 took 7.84245491027832\n",
      "Epoch:  1332\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1332 took 7.85701584815979\n",
      "Epoch:  1333\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1333 took 7.846233606338501\n",
      "Epoch:  1334\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1334 took 7.8443896770477295\n",
      "Epoch:  1335\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1335 took 7.846881628036499\n",
      "Epoch:  1336\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1336 took 7.848750352859497\n",
      "Epoch:  1337\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1337 took 7.883741140365601\n",
      "Epoch:  1338\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1338 took 7.849255084991455\n",
      "Epoch:  1339\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1339 took 7.867266654968262\n",
      "Epoch:  1340\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1340 took 7.853326797485352\n",
      "Epoch:  1341\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1341 took 7.853173494338989\n",
      "Epoch:  1342\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1342 took 7.854382514953613\n",
      "Epoch:  1343\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1343 took 7.8435516357421875\n",
      "Epoch:  1344\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1344 took 7.838589429855347\n",
      "Epoch:  1345\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1345 took 7.8456361293792725\n",
      "Epoch:  1346\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1346 took 7.82543158531189\n",
      "Epoch:  1347\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1347 took 7.848495006561279\n",
      "Epoch:  1348\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1348 took 7.84819221496582\n",
      "Epoch:  1349\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1349 took 7.858730316162109\n",
      "Epoch:  1350\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1350 took 7.863837957382202\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_1350.png\n",
      "Epoch:  1351\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1351 took 7.896462440490723\n",
      "Epoch:  1352\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1352 took 7.843172550201416\n",
      "Epoch:  1353\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1353 took 7.856095552444458\n",
      "Epoch:  1354\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1354 took 7.868601560592651\n",
      "Epoch:  1355\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1355 took 7.866804838180542\n",
      "Epoch:  1356\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1356 took 7.8527538776397705\n",
      "Epoch:  1357\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1357 took 7.842944860458374\n",
      "Epoch:  1358\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1358 took 7.845698118209839\n",
      "Epoch:  1359\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1359 took 7.852415561676025\n",
      "Epoch:  1360\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1360 took 7.835996866226196\n",
      "Epoch:  1361\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1361 took 7.841301202774048\n",
      "Epoch:  1362\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1362 took 7.847159147262573\n",
      "Epoch:  1363\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1363 took 7.846808195114136\n",
      "Epoch:  1364\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1364 took 7.864253997802734\n",
      "Epoch:  1365\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1365 took 7.849506616592407\n",
      "Epoch:  1366\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1366 took 7.84426474571228\n",
      "Epoch:  1367\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1367 took 7.851404428482056\n",
      "Epoch:  1368\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1368 took 7.847142219543457\n",
      "Epoch:  1369\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1369 took 7.87110447883606\n",
      "Epoch:  1370\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1370 took 7.864542245864868\n",
      "Epoch:  1371\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1371 took 7.861843109130859\n",
      "Epoch:  1372\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1372 took 7.85812783241272\n",
      "Epoch:  1373\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1373 took 7.836203575134277\n",
      "Epoch:  1374\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1374 took 7.84878134727478\n",
      "Epoch:  1375\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1375 took 7.838528633117676\n",
      "Epoch:  1376\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1376 took 7.8423216342926025\n",
      "Epoch:  1377\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1377 took 7.840826988220215\n",
      "Epoch:  1378\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1378 took 7.846698045730591\n",
      "Epoch:  1379\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1379 took 7.847844123840332\n",
      "Epoch:  1380\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1380 took 7.843449354171753\n",
      "Epoch:  1381\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1381 took 7.841583728790283\n",
      "Epoch:  1382\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1382 took 7.859316110610962\n",
      "Epoch:  1383\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1383 took 7.836779356002808\n",
      "Epoch:  1384\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1384 took 7.852167129516602\n",
      "Epoch:  1385\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1385 took 7.850757837295532\n",
      "Epoch:  1386\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1386 took 7.8461854457855225\n",
      "Epoch:  1387\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1387 took 7.850700855255127\n",
      "Epoch:  1388\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1388 took 7.842411279678345\n",
      "Epoch:  1389\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1389 took 7.8599770069122314\n",
      "Epoch:  1390\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1390 took 7.86775803565979\n",
      "Epoch:  1391\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1391 took 7.84033203125\n",
      "Epoch:  1392\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1392 took 7.859286785125732\n",
      "Epoch:  1393\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1393 took 7.864339590072632\n",
      "Epoch:  1394\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1394 took 7.842134237289429\n",
      "Epoch:  1395\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1395 took 7.857419967651367\n",
      "Epoch:  1396\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1396 took 7.850027084350586\n",
      "Epoch:  1397\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1397 took 7.8473289012908936\n",
      "Epoch:  1398\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1398 took 7.85008692741394\n",
      "Epoch:  1399\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1399 took 7.858339071273804\n",
      "Epoch:  1400\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1400 took 7.84697413444519\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_1400.png\n",
      "Epoch:  1401\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1401 took 7.87205696105957\n",
      "Epoch:  1402\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1402 took 7.842199325561523\n",
      "Epoch:  1403\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1403 took 7.8586745262146\n",
      "Epoch:  1404\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1404 took 7.866158962249756\n",
      "Epoch:  1405\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1405 took 7.878652572631836\n",
      "Epoch:  1406\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1406 took 7.858227014541626\n",
      "Epoch:  1407\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1407 took 7.850865125656128\n",
      "Epoch:  1408\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1408 took 7.86099100112915\n",
      "Epoch:  1409\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1409 took 7.858618497848511\n",
      "Epoch:  1410\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1410 took 7.842344284057617\n",
      "Epoch:  1411\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1411 took 7.838156461715698\n",
      "Epoch:  1412\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1412 took 7.840577602386475\n",
      "Epoch:  1413\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1413 took 7.83759617805481\n",
      "Epoch:  1414\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1414 took 7.845114231109619\n",
      "Epoch:  1415\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1415 took 7.857418060302734\n",
      "Epoch:  1416\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1416 took 7.84272313117981\n",
      "Epoch:  1417\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1417 took 7.850125074386597\n",
      "Epoch:  1418\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1418 took 7.848036527633667\n",
      "Epoch:  1419\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1419 took 7.843939542770386\n",
      "Epoch:  1420\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1420 took 7.84747052192688\n",
      "Epoch:  1421\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1421 took 7.849869012832642\n",
      "Epoch:  1422\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1422 took 7.848446607589722\n",
      "Epoch:  1423\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1423 took 7.837855100631714\n",
      "Epoch:  1424\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1424 took 7.8560779094696045\n",
      "Epoch:  1425\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1425 took 7.8475022315979\n",
      "Epoch:  1426\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1426 took 7.838759660720825\n",
      "Epoch:  1427\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1427 took 7.835632085800171\n",
      "Epoch:  1428\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1428 took 7.845136880874634\n",
      "Epoch:  1429\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1429 took 7.856343507766724\n",
      "Epoch:  1430\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1430 took 7.852361679077148\n",
      "Epoch:  1431\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1431 took 7.845249652862549\n",
      "Epoch:  1432\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1432 took 7.8607728481292725\n",
      "Epoch:  1433\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1433 took 7.8463356494903564\n",
      "Epoch:  1434\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1434 took 7.859360933303833\n",
      "Epoch:  1435\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1435 took 7.867315292358398\n",
      "Epoch:  1436\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1436 took 7.854121446609497\n",
      "Epoch:  1437\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1437 took 7.861336708068848\n",
      "Epoch:  1438\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1438 took 7.844514608383179\n",
      "Epoch:  1439\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1439 took 7.850125074386597\n",
      "Epoch:  1440\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1440 took 7.859306573867798\n",
      "Epoch:  1441\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1441 took 7.847686052322388\n",
      "Epoch:  1442\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1442 took 7.8390748500823975\n",
      "Epoch:  1443\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1443 took 7.8258957862854\n",
      "Epoch:  1444\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1444 took 7.835694789886475\n",
      "Epoch:  1445\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1445 took 7.845773220062256\n",
      "Epoch:  1446\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1446 took 7.85947585105896\n",
      "Epoch:  1447\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1447 took 7.845951318740845\n",
      "Epoch:  1448\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1448 took 7.857004642486572\n",
      "Epoch:  1449\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1449 took 7.871841669082642\n",
      "Epoch:  1450\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1450 took 7.857640981674194\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_1450.png\n",
      "Epoch:  1451\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1451 took 7.902313470840454\n",
      "Epoch:  1452\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1452 took 7.861165761947632\n",
      "Epoch:  1453\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1453 took 7.867450714111328\n",
      "Epoch:  1454\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1454 took 7.880116701126099\n",
      "Epoch:  1455\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1455 took 7.881330490112305\n",
      "Epoch:  1456\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1456 took 7.855127334594727\n",
      "Epoch:  1457\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1457 took 7.8627684116363525\n",
      "Epoch:  1458\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1458 took 7.847127914428711\n",
      "Epoch:  1459\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1459 took 7.855472087860107\n",
      "Epoch:  1460\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1460 took 7.863925457000732\n",
      "Epoch:  1461\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1461 took 7.838531494140625\n",
      "Epoch:  1462\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1462 took 7.838895559310913\n",
      "Epoch:  1463\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1463 took 7.849184274673462\n",
      "Epoch:  1464\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1464 took 7.865526914596558\n",
      "Epoch:  1465\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1465 took 7.847694635391235\n",
      "Epoch:  1466\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1466 took 7.838042974472046\n",
      "Epoch:  1467\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1467 took 7.840701103210449\n",
      "Epoch:  1468\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1468 took 7.853162050247192\n",
      "Epoch:  1469\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1469 took 7.858154296875\n",
      "Epoch:  1470\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1470 took 7.847213268280029\n",
      "Epoch:  1471\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1471 took 7.862476587295532\n",
      "Epoch:  1472\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1472 took 7.84922456741333\n",
      "Epoch:  1473\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1473 took 7.860640048980713\n",
      "Epoch:  1474\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1474 took 7.859435319900513\n",
      "Epoch:  1475\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1475 took 7.847028493881226\n",
      "Epoch:  1476\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1476 took 7.863689184188843\n",
      "Epoch:  1477\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1477 took 7.86130690574646\n",
      "Epoch:  1478\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1478 took 7.850592851638794\n",
      "Epoch:  1479\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1479 took 7.857781410217285\n",
      "Epoch:  1480\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1480 took 7.859056234359741\n",
      "Epoch:  1481\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1481 took 7.86540150642395\n",
      "Epoch:  1482\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1482 took 7.847622394561768\n",
      "Epoch:  1483\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1483 took 7.862870693206787\n",
      "Epoch:  1484\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1484 took 7.834611892700195\n",
      "Epoch:  1485\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1485 took 7.8454742431640625\n",
      "Epoch:  1486\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1486 took 7.859023571014404\n",
      "Epoch:  1487\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1487 took 7.85361647605896\n",
      "Epoch:  1488\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1488 took 7.861257076263428\n",
      "Epoch:  1489\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1489 took 7.840058088302612\n",
      "Epoch:  1490\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1490 took 7.837893724441528\n",
      "Epoch:  1491\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1491 took 7.851297855377197\n",
      "Epoch:  1492\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1492 took 7.842446804046631\n",
      "Epoch:  1493\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1493 took 7.84081768989563\n",
      "Epoch:  1494\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1494 took 7.840808868408203\n",
      "Epoch:  1495\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1495 took 7.8494343757629395\n",
      "Epoch:  1496\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1496 took 7.843082427978516\n",
      "Epoch:  1497\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1497 took 7.840978622436523\n",
      "Epoch:  1498\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1498 took 7.857177019119263\n",
      "Epoch:  1499\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1499 took 7.84279727935791\n",
      "Epoch:  1500\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1500 took 7.850414752960205\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_1500.png\n",
      "Epoch:  1501\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1501 took 7.877363681793213\n",
      "Epoch:  1502\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1502 took 7.855090856552124\n",
      "Epoch:  1503\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1503 took 7.870886564254761\n",
      "Epoch:  1504\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1504 took 7.884261608123779\n",
      "Epoch:  1505\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1505 took 7.879598617553711\n",
      "Epoch:  1506\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1506 took 7.862780570983887\n",
      "Epoch:  1507\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1507 took 7.8455283641815186\n",
      "Epoch:  1508\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1508 took 7.838343381881714\n",
      "Epoch:  1509\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1509 took 7.83075737953186\n",
      "Epoch:  1510\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1510 took 7.829394817352295\n",
      "Epoch:  1511\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1511 took 7.8358824253082275\n",
      "Epoch:  1512\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1512 took 7.8475282192230225\n",
      "Epoch:  1513\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1513 took 7.855231761932373\n",
      "Epoch:  1514\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1514 took 7.857807397842407\n",
      "Epoch:  1515\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1515 took 7.852243185043335\n",
      "Epoch:  1516\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1516 took 7.846283435821533\n",
      "Epoch:  1517\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1517 took 7.852685928344727\n",
      "Epoch:  1518\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1518 took 7.853087425231934\n",
      "Epoch:  1519\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1519 took 7.851139545440674\n",
      "Epoch:  1520\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1520 took 7.8657546043396\n",
      "Epoch:  1521\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1521 took 7.854375839233398\n",
      "Epoch:  1522\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1522 took 7.839421272277832\n",
      "Epoch:  1523\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1523 took 7.846174001693726\n",
      "Epoch:  1524\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1524 took 7.864172458648682\n",
      "Epoch:  1525\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1525 took 7.848616600036621\n",
      "Epoch:  1526\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1526 took 7.851495265960693\n",
      "Epoch:  1527\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1527 took 7.842791795730591\n",
      "Epoch:  1528\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1528 took 7.840797662734985\n",
      "Epoch:  1529\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1529 took 7.84036922454834\n",
      "Epoch:  1530\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1530 took 7.863886833190918\n",
      "Epoch:  1531\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1531 took 7.856447696685791\n",
      "Epoch:  1532\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1532 took 7.848691463470459\n",
      "Epoch:  1533\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1533 took 7.840255260467529\n",
      "Epoch:  1534\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1534 took 7.848403453826904\n",
      "Epoch:  1535\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1535 took 7.851095914840698\n",
      "Epoch:  1536\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1536 took 7.843409776687622\n",
      "Epoch:  1537\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1537 took 7.847485542297363\n",
      "Epoch:  1538\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1538 took 7.843658685684204\n",
      "Epoch:  1539\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1539 took 7.845448732376099\n",
      "Epoch:  1540\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1540 took 7.837037801742554\n",
      "Epoch:  1541\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1541 took 7.86339259147644\n",
      "Epoch:  1542\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1542 took 7.838576078414917\n",
      "Epoch:  1543\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1543 took 7.850553750991821\n",
      "Epoch:  1544\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1544 took 7.851630926132202\n",
      "Epoch:  1545\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1545 took 7.846139430999756\n",
      "Epoch:  1546\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1546 took 7.860553026199341\n",
      "Epoch:  1547\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1547 took 7.860358715057373\n",
      "Epoch:  1548\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1548 took 7.863959789276123\n",
      "Epoch:  1549\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1549 took 7.84731912612915\n",
      "Epoch:  1550\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1550 took 7.858959913253784\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_1550.png\n",
      "Epoch:  1551\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1551 took 7.876435995101929\n",
      "Epoch:  1552\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1552 took 7.853947401046753\n",
      "Epoch:  1553\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1553 took 7.876812219619751\n",
      "Epoch:  1554\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1554 took 7.877939462661743\n",
      "Epoch:  1555\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1555 took 7.865495443344116\n",
      "Epoch:  1556\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1556 took 7.855561256408691\n",
      "Epoch:  1557\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1557 took 7.856762170791626\n",
      "Epoch:  1558\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1558 took 7.852531433105469\n",
      "Epoch:  1559\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1559 took 7.845209360122681\n",
      "Epoch:  1560\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1560 took 7.846928358078003\n",
      "Epoch:  1561\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1561 took 7.847927808761597\n",
      "Epoch:  1562\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1562 took 7.861464738845825\n",
      "Epoch:  1563\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1563 took 7.854380369186401\n",
      "Epoch:  1564\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1564 took 7.86244535446167\n",
      "Epoch:  1565\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1565 took 7.848277568817139\n",
      "Epoch:  1566\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1566 took 7.8559348583221436\n",
      "Epoch:  1567\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1567 took 7.855187892913818\n",
      "Epoch:  1568\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1568 took 7.849472761154175\n",
      "Epoch:  1569\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1569 took 7.8614654541015625\n",
      "Epoch:  1570\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1570 took 7.847028732299805\n",
      "Epoch:  1571\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1571 took 7.850889205932617\n",
      "Epoch:  1572\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1572 took 7.8619585037231445\n",
      "Epoch:  1573\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1573 took 7.861055135726929\n",
      "Epoch:  1574\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1574 took 7.852591037750244\n",
      "Epoch:  1575\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1575 took 7.8477911949157715\n",
      "Epoch:  1576\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1576 took 7.8484838008880615\n",
      "Epoch:  1577\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1577 took 7.847231388092041\n",
      "Epoch:  1578\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1578 took 7.8620216846466064\n",
      "Epoch:  1579\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1579 took 7.8405396938323975\n",
      "Epoch:  1580\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1580 took 7.853838682174683\n",
      "Epoch:  1581\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1581 took 7.846503734588623\n",
      "Epoch:  1582\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1582 took 7.848724603652954\n",
      "Epoch:  1583\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1583 took 7.840440511703491\n",
      "Epoch:  1584\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1584 took 7.841365337371826\n",
      "Epoch:  1585\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1585 took 7.841171979904175\n",
      "Epoch:  1586\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1586 took 7.841711044311523\n",
      "Epoch:  1587\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1587 took 7.853896856307983\n",
      "Epoch:  1588\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1588 took 7.8456504344940186\n",
      "Epoch:  1589\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1589 took 7.848779916763306\n",
      "Epoch:  1590\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1590 took 7.859535455703735\n",
      "Epoch:  1591\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1591 took 7.855880498886108\n",
      "Epoch:  1592\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1592 took 7.852363586425781\n",
      "Epoch:  1593\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1593 took 7.85468316078186\n",
      "Epoch:  1594\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1594 took 7.860244989395142\n",
      "Epoch:  1595\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1595 took 7.855379819869995\n",
      "Epoch:  1596\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1596 took 7.859320402145386\n",
      "Epoch:  1597\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1597 took 7.846532106399536\n",
      "Epoch:  1598\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1598 took 7.843595027923584\n",
      "Epoch:  1599\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1599 took 7.858797311782837\n",
      "Epoch:  1600\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1600 took 7.844377279281616\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_1600.png\n",
      "Epoch:  1601\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1601 took 7.8985841274261475\n",
      "Epoch:  1602\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1602 took 7.860035181045532\n",
      "Epoch:  1603\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1603 took 7.902276515960693\n",
      "Epoch:  1604\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1604 took 7.892639636993408\n",
      "Epoch:  1605\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1605 took 7.865921497344971\n",
      "Epoch:  1606\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1606 took 7.852463006973267\n",
      "Epoch:  1607\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1607 took 7.87025260925293\n",
      "Epoch:  1608\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1608 took 7.846548318862915\n",
      "Epoch:  1609\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1609 took 7.829076766967773\n",
      "Epoch:  1610\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1610 took 7.847621440887451\n",
      "Epoch:  1611\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1611 took 7.847127676010132\n",
      "Epoch:  1612\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1612 took 7.832680702209473\n",
      "Epoch:  1613\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1613 took 7.843855857849121\n",
      "Epoch:  1614\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1614 took 7.867634296417236\n",
      "Epoch:  1615\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1615 took 7.846072673797607\n",
      "Epoch:  1616\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1616 took 7.85330605506897\n",
      "Epoch:  1617\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1617 took 7.847399473190308\n",
      "Epoch:  1618\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1618 took 7.838541030883789\n",
      "Epoch:  1619\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1619 took 7.858470439910889\n",
      "Epoch:  1620\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1620 took 7.852762222290039\n",
      "Epoch:  1621\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1621 took 7.8596179485321045\n",
      "Epoch:  1622\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1622 took 7.851495265960693\n",
      "Epoch:  1623\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1623 took 7.849509000778198\n",
      "Epoch:  1624\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1624 took 7.851983070373535\n",
      "Epoch:  1625\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1625 took 7.844946384429932\n",
      "Epoch:  1626\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1626 took 7.841311931610107\n",
      "Epoch:  1627\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1627 took 7.855712413787842\n",
      "Epoch:  1628\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1628 took 7.846878528594971\n",
      "Epoch:  1629\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1629 took 7.861042261123657\n",
      "Epoch:  1630\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1630 took 7.852515697479248\n",
      "Epoch:  1631\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1631 took 7.862014532089233\n",
      "Epoch:  1632\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1632 took 7.839592695236206\n",
      "Epoch:  1633\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1633 took 7.828619718551636\n",
      "Epoch:  1634\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1634 took 7.845903396606445\n",
      "Epoch:  1635\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1635 took 7.844508171081543\n",
      "Epoch:  1636\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1636 took 7.841840505599976\n",
      "Epoch:  1637\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1637 took 7.841585397720337\n",
      "Epoch:  1638\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1638 took 7.8608558177948\n",
      "Epoch:  1639\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1639 took 7.828222751617432\n",
      "Epoch:  1640\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1640 took 7.852309465408325\n",
      "Epoch:  1641\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1641 took 7.854280948638916\n",
      "Epoch:  1642\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1642 took 7.868326425552368\n",
      "Epoch:  1643\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1643 took 7.845765113830566\n",
      "Epoch:  1644\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1644 took 7.856231451034546\n",
      "Epoch:  1645\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1645 took 7.845348834991455\n",
      "Epoch:  1646\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1646 took 7.848933458328247\n",
      "Epoch:  1647\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1647 took 7.846223592758179\n",
      "Epoch:  1648\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1648 took 7.855010747909546\n",
      "Epoch:  1649\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1649 took 7.851738929748535\n",
      "Epoch:  1650\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1650 took 7.864635705947876\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_1650.png\n",
      "Epoch:  1651\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1651 took 7.879379510879517\n",
      "Epoch:  1652\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1652 took 7.846655368804932\n",
      "Epoch:  1653\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1653 took 7.8906426429748535\n",
      "Epoch:  1654\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1654 took 7.893331050872803\n",
      "Epoch:  1655\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1655 took 7.873119592666626\n",
      "Epoch:  1656\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1656 took 7.858106851577759\n",
      "Epoch:  1657\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1657 took 7.85798454284668\n",
      "Epoch:  1658\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1658 took 7.832553863525391\n",
      "Epoch:  1659\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1659 took 7.8375303745269775\n",
      "Epoch:  1660\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1660 took 7.848748683929443\n",
      "Epoch:  1661\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1661 took 7.849001169204712\n",
      "Epoch:  1662\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1662 took 7.8392908573150635\n",
      "Epoch:  1663\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1663 took 7.854649066925049\n",
      "Epoch:  1664\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1664 took 7.8466737270355225\n",
      "Epoch:  1665\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1665 took 7.850686550140381\n",
      "Epoch:  1666\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1666 took 7.859954357147217\n",
      "Epoch:  1667\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1667 took 7.8567681312561035\n",
      "Epoch:  1668\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1668 took 7.8564372062683105\n",
      "Epoch:  1669\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1669 took 7.854909896850586\n",
      "Epoch:  1670\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1670 took 7.852251052856445\n",
      "Epoch:  1671\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1671 took 7.846540927886963\n",
      "Epoch:  1672\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1672 took 7.847959756851196\n",
      "Epoch:  1673\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1673 took 7.845602512359619\n",
      "Epoch:  1674\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1674 took 7.843547344207764\n",
      "Epoch:  1675\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1675 took 7.853082656860352\n",
      "Epoch:  1676\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1676 took 7.863602876663208\n",
      "Epoch:  1677\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1677 took 7.843963861465454\n",
      "Epoch:  1678\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1678 took 7.862758159637451\n",
      "Epoch:  1679\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1679 took 7.855849742889404\n",
      "Epoch:  1680\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1680 took 7.852285146713257\n",
      "Epoch:  1681\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1681 took 7.857317686080933\n",
      "Epoch:  1682\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1682 took 7.848597764968872\n",
      "Epoch:  1683\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1683 took 7.851675510406494\n",
      "Epoch:  1684\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1684 took 7.844322919845581\n",
      "Epoch:  1685\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1685 took 7.847994565963745\n",
      "Epoch:  1686\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1686 took 7.8490660190582275\n",
      "Epoch:  1687\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1687 took 7.8468239307403564\n",
      "Epoch:  1688\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1688 took 7.83506441116333\n",
      "Epoch:  1689\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1689 took 7.8652184009552\n",
      "Epoch:  1690\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1690 took 7.8441362380981445\n",
      "Epoch:  1691\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1691 took 7.841475486755371\n",
      "Epoch:  1692\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1692 took 7.843782901763916\n",
      "Epoch:  1693\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1693 took 7.838609218597412\n",
      "Epoch:  1694\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1694 took 7.854472637176514\n",
      "Epoch:  1695\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1695 took 7.850546360015869\n",
      "Epoch:  1696\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1696 took 7.832650661468506\n",
      "Epoch:  1697\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1697 took 7.839799404144287\n",
      "Epoch:  1698\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1698 took 7.856228351593018\n",
      "Epoch:  1699\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1699 took 7.848822116851807\n",
      "Epoch:  1700\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1700 took 7.860588312149048\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_1700.png\n",
      "Epoch:  1701\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1701 took 7.901525020599365\n",
      "Epoch:  1702\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1702 took 7.854609966278076\n",
      "Epoch:  1703\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1703 took 7.880231142044067\n",
      "Epoch:  1704\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1704 took 7.886910915374756\n",
      "Epoch:  1705\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1705 took 7.868041038513184\n",
      "Epoch:  1706\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1706 took 7.854692459106445\n",
      "Epoch:  1707\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1707 took 7.847160816192627\n",
      "Epoch:  1708\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1708 took 7.8497655391693115\n",
      "Epoch:  1709\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1709 took 7.840110778808594\n",
      "Epoch:  1710\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1710 took 7.842209100723267\n",
      "Epoch:  1711\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1711 took 7.847398042678833\n",
      "Epoch:  1712\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1712 took 7.847620725631714\n",
      "Epoch:  1713\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1713 took 7.84719181060791\n",
      "Epoch:  1714\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1714 took 7.846923589706421\n",
      "Epoch:  1715\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1715 took 7.842444658279419\n",
      "Epoch:  1716\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1716 took 7.839212656021118\n",
      "Epoch:  1717\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1717 took 7.845025539398193\n",
      "Epoch:  1718\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1718 took 7.854865789413452\n",
      "Epoch:  1719\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1719 took 7.8592047691345215\n",
      "Epoch:  1720\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1720 took 7.851800203323364\n",
      "Epoch:  1721\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1721 took 7.851019620895386\n",
      "Epoch:  1722\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1722 took 7.862677574157715\n",
      "Epoch:  1723\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1723 took 7.863119602203369\n",
      "Epoch:  1724\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1724 took 7.842648983001709\n",
      "Epoch:  1725\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1725 took 7.858626842498779\n",
      "Epoch:  1726\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1726 took 7.854322195053101\n",
      "Epoch:  1727\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1727 took 7.849361896514893\n",
      "Epoch:  1728\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1728 took 7.836261987686157\n",
      "Epoch:  1729\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1729 took 7.843695402145386\n",
      "Epoch:  1730\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1730 took 7.85757303237915\n",
      "Epoch:  1731\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1731 took 7.85311222076416\n",
      "Epoch:  1732\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1732 took 7.856391906738281\n",
      "Epoch:  1733\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1733 took 7.847378492355347\n",
      "Epoch:  1734\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1734 took 7.841084718704224\n",
      "Epoch:  1735\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1735 took 7.8454179763793945\n",
      "Epoch:  1736\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1736 took 7.8542585372924805\n",
      "Epoch:  1737\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1737 took 7.8409950733184814\n",
      "Epoch:  1738\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1738 took 7.847180366516113\n",
      "Epoch:  1739\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1739 took 7.8326733112335205\n",
      "Epoch:  1740\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1740 took 7.852632522583008\n",
      "Epoch:  1741\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1741 took 7.8551881313323975\n",
      "Epoch:  1742\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1742 took 7.862649440765381\n",
      "Epoch:  1743\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1743 took 7.848162651062012\n",
      "Epoch:  1744\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1744 took 7.849433660507202\n",
      "Epoch:  1745\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1745 took 7.834099054336548\n",
      "Epoch:  1746\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1746 took 7.863871335983276\n",
      "Epoch:  1747\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1747 took 7.850260019302368\n",
      "Epoch:  1748\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1748 took 7.850794792175293\n",
      "Epoch:  1749\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1749 took 7.848055124282837\n",
      "Epoch:  1750\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1750 took 7.836632013320923\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_1750.png\n",
      "Epoch:  1751\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1751 took 7.902318000793457\n",
      "Epoch:  1752\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1752 took 7.847326278686523\n",
      "Epoch:  1753\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1753 took 7.885273694992065\n",
      "Epoch:  1754\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1754 took 7.889210224151611\n",
      "Epoch:  1755\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1755 took 7.873871088027954\n",
      "Epoch:  1756\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1756 took 7.854107618331909\n",
      "Epoch:  1757\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1757 took 7.855633974075317\n",
      "Epoch:  1758\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1758 took 7.856176853179932\n",
      "Epoch:  1759\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1759 took 7.828848361968994\n",
      "Epoch:  1760\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1760 took 7.8469157218933105\n",
      "Epoch:  1761\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1761 took 7.8562023639678955\n",
      "Epoch:  1762\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1762 took 7.840804576873779\n",
      "Epoch:  1763\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1763 took 7.851233959197998\n",
      "Epoch:  1764\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1764 took 7.856475591659546\n",
      "Epoch:  1765\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1765 took 7.844827890396118\n",
      "Epoch:  1766\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1766 took 7.864360570907593\n",
      "Epoch:  1767\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1767 took 7.8481996059417725\n",
      "Epoch:  1768\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1768 took 7.858083248138428\n",
      "Epoch:  1769\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1769 took 7.8558714389801025\n",
      "Epoch:  1770\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1770 took 7.850017786026001\n",
      "Epoch:  1771\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1771 took 7.8563392162323\n",
      "Epoch:  1772\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1772 took 7.852585077285767\n",
      "Epoch:  1773\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1773 took 7.845059394836426\n",
      "Epoch:  1774\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1774 took 7.84744119644165\n",
      "Epoch:  1775\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1775 took 7.845728397369385\n",
      "Epoch:  1776\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1776 took 7.852625131607056\n",
      "Epoch:  1777\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1777 took 7.843745470046997\n",
      "Epoch:  1778\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1778 took 7.84235405921936\n",
      "Epoch:  1779\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1779 took 7.839290618896484\n",
      "Epoch:  1780\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1780 took 7.841771364212036\n",
      "Epoch:  1781\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1781 took 7.8489720821380615\n",
      "Epoch:  1782\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1782 took 7.844735145568848\n",
      "Epoch:  1783\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1783 took 7.854095935821533\n",
      "Epoch:  1784\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1784 took 7.852588176727295\n",
      "Epoch:  1785\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1785 took 7.855613470077515\n",
      "Epoch:  1786\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1786 took 7.83809757232666\n",
      "Epoch:  1787\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1787 took 7.843631267547607\n",
      "Epoch:  1788\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1788 took 7.865232467651367\n",
      "Epoch:  1789\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1789 took 7.854699611663818\n",
      "Epoch:  1790\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1790 took 7.854287147521973\n",
      "Epoch:  1791\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1791 took 7.857621431350708\n",
      "Epoch:  1792\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1792 took 7.852200269699097\n",
      "Epoch:  1793\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1793 took 7.853296995162964\n",
      "Epoch:  1794\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1794 took 7.844666242599487\n",
      "Epoch:  1795\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1795 took 7.8537163734436035\n",
      "Epoch:  1796\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1796 took 7.852545976638794\n",
      "Epoch:  1797\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1797 took 7.864508390426636\n",
      "Epoch:  1798\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1798 took 7.8537280559539795\n",
      "Epoch:  1799\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1799 took 7.838037014007568\n",
      "Epoch:  1800\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1800 took 7.84121036529541\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_1800.png\n",
      "Epoch:  1801\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1801 took 7.877671003341675\n",
      "Epoch:  1802\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1802 took 7.847999811172485\n",
      "Epoch:  1803\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1803 took 7.870095729827881\n",
      "Epoch:  1804\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1804 took 7.887801647186279\n",
      "Epoch:  1805\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1805 took 7.877153396606445\n",
      "Epoch:  1806\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1806 took 7.861936807632446\n",
      "Epoch:  1807\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1807 took 7.857943773269653\n",
      "Epoch:  1808\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1808 took 7.848429203033447\n",
      "Epoch:  1809\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1809 took 7.83954644203186\n",
      "Epoch:  1810\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1810 took 7.849358320236206\n",
      "Epoch:  1811\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1811 took 7.8630053997039795\n",
      "Epoch:  1812\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1812 took 7.851919174194336\n",
      "Epoch:  1813\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1813 took 7.844511985778809\n",
      "Epoch:  1814\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1814 took 7.846004009246826\n",
      "Epoch:  1815\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1815 took 7.848739862442017\n",
      "Epoch:  1816\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1816 took 7.848891735076904\n",
      "Epoch:  1817\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1817 took 7.8539605140686035\n",
      "Epoch:  1818\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1818 took 7.86134934425354\n",
      "Epoch:  1819\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1819 took 7.849792003631592\n",
      "Epoch:  1820\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1820 took 7.840921640396118\n",
      "Epoch:  1821\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1821 took 7.852727890014648\n",
      "Epoch:  1822\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1822 took 7.855437517166138\n",
      "Epoch:  1823\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1823 took 7.84167742729187\n",
      "Epoch:  1824\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1824 took 7.845964193344116\n",
      "Epoch:  1825\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1825 took 7.839163780212402\n",
      "Epoch:  1826\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1826 took 7.853475332260132\n",
      "Epoch:  1827\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1827 took 7.84153151512146\n",
      "Epoch:  1828\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1828 took 7.85086464881897\n",
      "Epoch:  1829\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1829 took 7.8500964641571045\n",
      "Epoch:  1830\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1830 took 7.84187912940979\n",
      "Epoch:  1831\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1831 took 7.849386930465698\n",
      "Epoch:  1832\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1832 took 7.849954843521118\n",
      "Epoch:  1833\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1833 took 7.8492820262908936\n",
      "Epoch:  1834\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1834 took 7.842980861663818\n",
      "Epoch:  1835\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1835 took 7.84464168548584\n",
      "Epoch:  1836\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1836 took 7.86620020866394\n",
      "Epoch:  1837\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1837 took 7.847628593444824\n",
      "Epoch:  1838\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1838 took 7.839507579803467\n",
      "Epoch:  1839\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1839 took 7.842272996902466\n",
      "Epoch:  1840\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1840 took 7.835582494735718\n",
      "Epoch:  1841\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1841 took 7.85416054725647\n",
      "Epoch:  1842\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1842 took 7.837163209915161\n",
      "Epoch:  1843\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1843 took 7.84514856338501\n",
      "Epoch:  1844\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1844 took 7.8428826332092285\n",
      "Epoch:  1845\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1845 took 7.838993549346924\n",
      "Epoch:  1846\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1846 took 7.838340520858765\n",
      "Epoch:  1847\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1847 took 7.83805251121521\n",
      "Epoch:  1848\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1848 took 7.855520009994507\n",
      "Epoch:  1849\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1849 took 7.848942279815674\n",
      "Epoch:  1850\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1850 took 7.861837148666382\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_1850.png\n",
      "Epoch:  1851\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1851 took 7.874798536300659\n",
      "Epoch:  1852\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1852 took 7.847801923751831\n",
      "Epoch:  1853\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1853 took 7.886250257492065\n",
      "Epoch:  1854\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1854 took 7.890452146530151\n",
      "Epoch:  1855\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1855 took 7.872760772705078\n",
      "Epoch:  1856\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1856 took 7.868289947509766\n",
      "Epoch:  1857\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1857 took 7.864150524139404\n",
      "Epoch:  1858\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1858 took 7.8340959548950195\n",
      "Epoch:  1859\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1859 took 7.829131603240967\n",
      "Epoch:  1860\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1860 took 7.842564344406128\n",
      "Epoch:  1861\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1861 took 7.843734264373779\n",
      "Epoch:  1862\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1862 took 7.840059995651245\n",
      "Epoch:  1863\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1863 took 7.859976768493652\n",
      "Epoch:  1864\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1864 took 7.854133605957031\n",
      "Epoch:  1865\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1865 took 7.868897199630737\n",
      "Epoch:  1866\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1866 took 7.860926389694214\n",
      "Epoch:  1867\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1867 took 7.871308088302612\n",
      "Epoch:  1868\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1868 took 7.858965873718262\n",
      "Epoch:  1869\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1869 took 7.851806879043579\n",
      "Epoch:  1870\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1870 took 7.8674657344818115\n",
      "Epoch:  1871\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1871 took 7.859255075454712\n",
      "Epoch:  1872\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1872 took 7.847880840301514\n",
      "Epoch:  1873\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1873 took 7.858307600021362\n",
      "Epoch:  1874\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1874 took 7.87202525138855\n",
      "Epoch:  1875\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1875 took 7.864587068557739\n",
      "Epoch:  1876\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1876 took 7.856625080108643\n",
      "Epoch:  1877\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1877 took 7.846352815628052\n",
      "Epoch:  1878\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1878 took 7.840906143188477\n",
      "Epoch:  1879\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1879 took 7.853255033493042\n",
      "Epoch:  1880\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1880 took 7.859503269195557\n",
      "Epoch:  1881\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1881 took 7.855437994003296\n",
      "Epoch:  1882\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1882 took 7.848106861114502\n",
      "Epoch:  1883\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1883 took 7.844637632369995\n",
      "Epoch:  1884\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1884 took 7.851717948913574\n",
      "Epoch:  1885\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1885 took 7.856375455856323\n",
      "Epoch:  1886\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1886 took 7.836704969406128\n",
      "Epoch:  1887\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1887 took 7.852290391921997\n",
      "Epoch:  1888\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1888 took 7.844303369522095\n",
      "Epoch:  1889\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1889 took 7.841233730316162\n",
      "Epoch:  1890\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1890 took 7.851169586181641\n",
      "Epoch:  1891\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1891 took 7.8630359172821045\n",
      "Epoch:  1892\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1892 took 7.849703550338745\n",
      "Epoch:  1893\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1893 took 7.849576950073242\n",
      "Epoch:  1894\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1894 took 7.849579572677612\n",
      "Epoch:  1895\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1895 took 7.858670473098755\n",
      "Epoch:  1896\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1896 took 7.862531900405884\n",
      "Epoch:  1897\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1897 took 7.8503477573394775\n",
      "Epoch:  1898\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1898 took 7.8514862060546875\n",
      "Epoch:  1899\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1899 took 7.841942071914673\n",
      "Epoch:  1900\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1900 took 7.8556623458862305\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_1900.png\n",
      "Epoch:  1901\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1901 took 7.8655009269714355\n",
      "Epoch:  1902\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1902 took 7.853306293487549\n",
      "Epoch:  1903\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1903 took 7.884649753570557\n",
      "Epoch:  1904\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1904 took 7.898924827575684\n",
      "Epoch:  1905\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1905 took 7.879537105560303\n",
      "Epoch:  1906\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1906 took 7.866608381271362\n",
      "Epoch:  1907\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1907 took 7.84613037109375\n",
      "Epoch:  1908\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1908 took 7.844584226608276\n",
      "Epoch:  1909\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1909 took 7.83879280090332\n",
      "Epoch:  1910\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1910 took 7.831017017364502\n",
      "Epoch:  1911\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1911 took 7.854466915130615\n",
      "Epoch:  1912\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1912 took 7.851746559143066\n",
      "Epoch:  1913\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1913 took 7.855690002441406\n",
      "Epoch:  1914\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1914 took 7.849277019500732\n",
      "Epoch:  1915\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1915 took 7.84888768196106\n",
      "Epoch:  1916\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1916 took 7.857034206390381\n",
      "Epoch:  1917\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1917 took 7.859900712966919\n",
      "Epoch:  1918\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1918 took 7.8642072677612305\n",
      "Epoch:  1919\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1919 took 7.8568456172943115\n",
      "Epoch:  1920\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1920 took 7.861695766448975\n",
      "Epoch:  1921\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1921 took 7.860589265823364\n",
      "Epoch:  1922\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1922 took 7.8650617599487305\n",
      "Epoch:  1923\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1923 took 7.862484455108643\n",
      "Epoch:  1924\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1924 took 7.848923444747925\n",
      "Epoch:  1925\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1925 took 7.856595516204834\n",
      "Epoch:  1926\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1926 took 7.838682174682617\n",
      "Epoch:  1927\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1927 took 7.856588840484619\n",
      "Epoch:  1928\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1928 took 7.83510160446167\n",
      "Epoch:  1929\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1929 took 7.849362134933472\n",
      "Epoch:  1930\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1930 took 7.837010145187378\n",
      "Epoch:  1931\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1931 took 7.857537269592285\n",
      "Epoch:  1932\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1932 took 7.831762075424194\n",
      "Epoch:  1933\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1933 took 7.852644205093384\n",
      "Epoch:  1934\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1934 took 7.859446048736572\n",
      "Epoch:  1935\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1935 took 7.843378782272339\n",
      "Epoch:  1936\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1936 took 7.842280864715576\n",
      "Epoch:  1937\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1937 took 7.848861455917358\n",
      "Epoch:  1938\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1938 took 7.841335296630859\n",
      "Epoch:  1939\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1939 took 7.84540581703186\n",
      "Epoch:  1940\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1940 took 7.864827394485474\n",
      "Epoch:  1941\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1941 took 7.8447675704956055\n",
      "Epoch:  1942\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1942 took 7.8471901416778564\n",
      "Epoch:  1943\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1943 took 7.852130651473999\n",
      "Epoch:  1944\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1944 took 7.865908861160278\n",
      "Epoch:  1945\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1945 took 7.847236156463623\n",
      "Epoch:  1946\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1946 took 7.851032733917236\n",
      "Epoch:  1947\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1947 took 7.842947959899902\n",
      "Epoch:  1948\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1948 took 7.841230630874634\n",
      "Epoch:  1949\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1949 took 7.846928119659424\n",
      "Epoch:  1950\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1950 took 7.854712963104248\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_1950.png\n",
      "Epoch:  1951\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1951 took 7.889760255813599\n",
      "Epoch:  1952\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1952 took 7.857670783996582\n",
      "Epoch:  1953\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1953 took 7.880819797515869\n",
      "Epoch:  1954\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1954 took 7.889782667160034\n",
      "Epoch:  1955\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1955 took 7.855928659439087\n",
      "Epoch:  1956\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1956 took 7.86425518989563\n",
      "Epoch:  1957\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1957 took 7.850372076034546\n",
      "Epoch:  1958\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1958 took 7.846346855163574\n",
      "Epoch:  1959\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1959 took 7.828598737716675\n",
      "Epoch:  1960\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1960 took 7.837700128555298\n",
      "Epoch:  1961\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1961 took 7.8483967781066895\n",
      "Epoch:  1962\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1962 took 7.842913627624512\n",
      "Epoch:  1963\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1963 took 7.855295896530151\n",
      "Epoch:  1964\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1964 took 7.852828502655029\n",
      "Epoch:  1965\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1965 took 7.854165077209473\n",
      "Epoch:  1966\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1966 took 7.856290817260742\n",
      "Epoch:  1967\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1967 took 7.862194538116455\n",
      "Epoch:  1968\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1968 took 7.85096001625061\n",
      "Epoch:  1969\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1969 took 7.849327564239502\n",
      "Epoch:  1970\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1970 took 7.8519768714904785\n",
      "Epoch:  1971\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1971 took 7.849330186843872\n",
      "Epoch:  1972\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1972 took 7.8709070682525635\n",
      "Epoch:  1973\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1973 took 7.8465001583099365\n",
      "Epoch:  1974\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1974 took 7.8443121910095215\n",
      "Epoch:  1975\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1975 took 7.8556108474731445\n",
      "Epoch:  1976\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1976 took 7.847297191619873\n",
      "Epoch:  1977\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1977 took 7.845043182373047\n",
      "Epoch:  1978\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1978 took 7.849865198135376\n",
      "Epoch:  1979\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1979 took 7.849472999572754\n",
      "Epoch:  1980\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1980 took 7.846785068511963\n",
      "Epoch:  1981\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1981 took 7.864521026611328\n",
      "Epoch:  1982\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1982 took 7.851337194442749\n",
      "Epoch:  1983\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1983 took 7.842937231063843\n",
      "Epoch:  1984\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1984 took 7.852909564971924\n",
      "Epoch:  1985\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1985 took 7.844974517822266\n",
      "Epoch:  1986\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1986 took 7.847201108932495\n",
      "Epoch:  1987\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1987 took 7.844488143920898\n",
      "Epoch:  1988\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1988 took 7.839502334594727\n",
      "Epoch:  1989\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1989 took 7.838292121887207\n",
      "Epoch:  1990\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1990 took 7.851437330245972\n",
      "Epoch:  1991\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1991 took 7.855621576309204\n",
      "Epoch:  1992\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1992 took 7.842785358428955\n",
      "Epoch:  1993\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1993 took 7.843948841094971\n",
      "Epoch:  1994\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1994 took 7.835363388061523\n",
      "Epoch:  1995\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1995 took 7.8384199142456055\n",
      "Epoch:  1996\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1996 took 7.836826324462891\n",
      "Epoch:  1997\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1997 took 7.850548505783081\n",
      "Epoch:  1998\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1998 took 7.845050573348999\n",
      "Epoch:  1999\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 1999 took 7.8503289222717285\n",
      "Epoch:  2000\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2000 took 7.83206844329834\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_2000.png\n",
      "Epoch:  2001\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2001 took 7.87476110458374\n",
      "Epoch:  2002\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2002 took 7.859984636306763\n",
      "Epoch:  2003\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2003 took 7.8668372631073\n",
      "Epoch:  2004\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2004 took 7.884707689285278\n",
      "Epoch:  2005\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2005 took 7.874277353286743\n",
      "Epoch:  2006\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2006 took 7.8640968799591064\n",
      "Epoch:  2007\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2007 took 7.841124773025513\n",
      "Epoch:  2008\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2008 took 7.8333351612091064\n",
      "Epoch:  2009\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2009 took 7.839248418807983\n",
      "Epoch:  2010\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2010 took 7.830835342407227\n",
      "Epoch:  2011\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2011 took 7.829993486404419\n",
      "Epoch:  2012\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2012 took 7.8468217849731445\n",
      "Epoch:  2013\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2013 took 7.833956003189087\n",
      "Epoch:  2014\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2014 took 7.838134527206421\n",
      "Epoch:  2015\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2015 took 7.863386869430542\n",
      "Epoch:  2016\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2016 took 7.838266849517822\n",
      "Epoch:  2017\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2017 took 7.853020191192627\n",
      "Epoch:  2018\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2018 took 7.8442747592926025\n",
      "Epoch:  2019\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2019 took 7.853839159011841\n",
      "Epoch:  2020\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2020 took 7.8442466259002686\n",
      "Epoch:  2021\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2021 took 7.855747938156128\n",
      "Epoch:  2022\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2022 took 7.8554041385650635\n",
      "Epoch:  2023\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2023 took 7.862786769866943\n",
      "Epoch:  2024\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2024 took 7.855996608734131\n",
      "Epoch:  2025\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2025 took 7.84617018699646\n",
      "Epoch:  2026\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2026 took 7.853115081787109\n",
      "Epoch:  2027\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2027 took 7.848890542984009\n",
      "Epoch:  2028\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2028 took 7.843474388122559\n",
      "Epoch:  2029\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2029 took 7.87164306640625\n",
      "Epoch:  2030\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2030 took 7.8657567501068115\n",
      "Epoch:  2031\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2031 took 7.854128122329712\n",
      "Epoch:  2032\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2032 took 7.846463203430176\n",
      "Epoch:  2033\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2033 took 7.846420049667358\n",
      "Epoch:  2034\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2034 took 7.83064603805542\n",
      "Epoch:  2035\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2035 took 7.854094743728638\n",
      "Epoch:  2036\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2036 took 7.847466230392456\n",
      "Epoch:  2037\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2037 took 7.840080499649048\n",
      "Epoch:  2038\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2038 took 7.841362237930298\n",
      "Epoch:  2039\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2039 took 7.836587905883789\n",
      "Epoch:  2040\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2040 took 7.849911451339722\n",
      "Epoch:  2041\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2041 took 7.853607654571533\n",
      "Epoch:  2042\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2042 took 7.854068279266357\n",
      "Epoch:  2043\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2043 took 7.851987361907959\n",
      "Epoch:  2044\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2044 took 7.843581914901733\n",
      "Epoch:  2045\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2045 took 7.85010552406311\n",
      "Epoch:  2046\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2046 took 7.85405969619751\n",
      "Epoch:  2047\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2047 took 7.856867074966431\n",
      "Epoch:  2048\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2048 took 7.84813666343689\n",
      "Epoch:  2049\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2049 took 7.845750093460083\n",
      "Epoch:  2050\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2050 took 7.83798360824585\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_2050.png\n",
      "Epoch:  2051\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2051 took 7.881732225418091\n",
      "Epoch:  2052\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2052 took 7.852001667022705\n",
      "Epoch:  2053\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2053 took 7.9016242027282715\n",
      "Epoch:  2054\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2054 took 7.894484281539917\n",
      "Epoch:  2055\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2055 took 7.867919921875\n",
      "Epoch:  2056\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2056 took 7.860488176345825\n",
      "Epoch:  2057\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2057 took 7.854419231414795\n",
      "Epoch:  2058\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2058 took 7.844515562057495\n",
      "Epoch:  2059\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2059 took 7.837177991867065\n",
      "Epoch:  2060\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2060 took 7.8348708152771\n",
      "Epoch:  2061\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2061 took 7.835227727890015\n",
      "Epoch:  2062\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2062 took 7.8599889278411865\n",
      "Epoch:  2063\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2063 took 7.839917421340942\n",
      "Epoch:  2064\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2064 took 7.851308107376099\n",
      "Epoch:  2065\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2065 took 7.843465328216553\n",
      "Epoch:  2066\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2066 took 7.852400064468384\n",
      "Epoch:  2067\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2067 took 7.865142345428467\n",
      "Epoch:  2068\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2068 took 7.866912126541138\n",
      "Epoch:  2069\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2069 took 7.850787401199341\n",
      "Epoch:  2070\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2070 took 7.853595495223999\n",
      "Epoch:  2071\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2071 took 7.859691619873047\n",
      "Epoch:  2072\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2072 took 7.860268831253052\n",
      "Epoch:  2073\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2073 took 7.852738618850708\n",
      "Epoch:  2074\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2074 took 7.855778694152832\n",
      "Epoch:  2075\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2075 took 7.864034414291382\n",
      "Epoch:  2076\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2076 took 7.855648040771484\n",
      "Epoch:  2077\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2077 took 7.864322185516357\n",
      "Epoch:  2078\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2078 took 7.847832679748535\n",
      "Epoch:  2079\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2079 took 7.858566522598267\n",
      "Epoch:  2080\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2080 took 7.8473451137542725\n",
      "Epoch:  2081\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2081 took 7.84143328666687\n",
      "Epoch:  2082\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2082 took 7.8430397510528564\n",
      "Epoch:  2083\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2083 took 7.843695402145386\n",
      "Epoch:  2084\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2084 took 7.8447864055633545\n",
      "Epoch:  2085\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2085 took 7.841909408569336\n",
      "Epoch:  2086\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2086 took 7.848039865493774\n",
      "Epoch:  2087\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2087 took 7.848446607589722\n",
      "Epoch:  2088\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2088 took 7.855497598648071\n",
      "Epoch:  2089\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2089 took 7.849088668823242\n",
      "Epoch:  2090\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2090 took 7.848210334777832\n",
      "Epoch:  2091\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2091 took 7.850537300109863\n",
      "Epoch:  2092\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2092 took 7.8566038608551025\n",
      "Epoch:  2093\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2093 took 7.850034236907959\n",
      "Epoch:  2094\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2094 took 7.8529064655303955\n",
      "Epoch:  2095\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2095 took 7.841834306716919\n",
      "Epoch:  2096\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2096 took 7.848872423171997\n",
      "Epoch:  2097\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2097 took 7.842405319213867\n",
      "Epoch:  2098\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2098 took 7.847793102264404\n",
      "Epoch:  2099\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2099 took 7.842473030090332\n",
      "Epoch:  2100\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2100 took 7.853088140487671\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_2100.png\n",
      "Epoch:  2101\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2101 took 7.862070560455322\n",
      "Epoch:  2102\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2102 took 7.850350379943848\n",
      "Epoch:  2103\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2103 took 7.866544008255005\n",
      "Epoch:  2104\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2104 took 7.870494365692139\n",
      "Epoch:  2105\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2105 took 7.85833215713501\n",
      "Epoch:  2106\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2106 took 7.870059490203857\n",
      "Epoch:  2107\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2107 took 7.8756139278411865\n",
      "Epoch:  2108\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2108 took 7.850522756576538\n",
      "Epoch:  2109\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2109 took 7.8437957763671875\n",
      "Epoch:  2110\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2110 took 7.834163427352905\n",
      "Epoch:  2111\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2111 took 7.84026837348938\n",
      "Epoch:  2112\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2112 took 7.854438543319702\n",
      "Epoch:  2113\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2113 took 7.85236930847168\n",
      "Epoch:  2114\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2114 took 7.851763725280762\n",
      "Epoch:  2115\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2115 took 7.849412202835083\n",
      "Epoch:  2116\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2116 took 7.847295045852661\n",
      "Epoch:  2117\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2117 took 7.859039545059204\n",
      "Epoch:  2118\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2118 took 7.848952293395996\n",
      "Epoch:  2119\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2119 took 7.857824325561523\n",
      "Epoch:  2120\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2120 took 7.854176044464111\n",
      "Epoch:  2121\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2121 took 7.853392124176025\n",
      "Epoch:  2122\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2122 took 7.843836069107056\n",
      "Epoch:  2123\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2123 took 7.8672778606414795\n",
      "Epoch:  2124\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2124 took 7.835456371307373\n",
      "Epoch:  2125\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2125 took 7.844484090805054\n",
      "Epoch:  2126\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2126 took 7.852824687957764\n",
      "Epoch:  2127\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2127 took 7.841228008270264\n",
      "Epoch:  2128\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2128 took 7.84349513053894\n",
      "Epoch:  2129\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2129 took 7.850371837615967\n",
      "Epoch:  2130\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2130 took 7.838630199432373\n",
      "Epoch:  2131\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2131 took 7.8361656665802\n",
      "Epoch:  2132\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2132 took 7.836627006530762\n",
      "Epoch:  2133\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2133 took 7.8464860916137695\n",
      "Epoch:  2134\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2134 took 7.851369380950928\n",
      "Epoch:  2135\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2135 took 7.861225128173828\n",
      "Epoch:  2136\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2136 took 7.840850830078125\n",
      "Epoch:  2137\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2137 took 7.839455604553223\n",
      "Epoch:  2138\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2138 took 7.849781036376953\n",
      "Epoch:  2139\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2139 took 7.842050552368164\n",
      "Epoch:  2140\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2140 took 7.8494873046875\n",
      "Epoch:  2141\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2141 took 7.850191116333008\n",
      "Epoch:  2142\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2142 took 7.861708402633667\n",
      "Epoch:  2143\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2143 took 7.848645925521851\n",
      "Epoch:  2144\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2144 took 7.856417894363403\n",
      "Epoch:  2145\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2145 took 7.848862648010254\n",
      "Epoch:  2146\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2146 took 7.850457668304443\n",
      "Epoch:  2147\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2147 took 7.848864316940308\n",
      "Epoch:  2148\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2148 took 7.850898504257202\n",
      "Epoch:  2149\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2149 took 7.837014675140381\n",
      "Epoch:  2150\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2150 took 7.854938268661499\n",
      "Saving weights\n",
      "(100, 256, 256, 3)\n",
      "output/T1/epoch_2150.png\n",
      "Epoch:  2151\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2151 took 7.8700010776519775\n",
      "Epoch:  2152\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2152 took 7.858510971069336\n",
      "Epoch:  2153\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2153 took 7.872150421142578\n",
      "Epoch:  2154\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2154 took 7.8852667808532715\n",
      "Epoch:  2155\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2155 took 7.881727695465088\n",
      "Epoch:  2156\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2156 took 7.861054182052612\n",
      "Epoch:  2157\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2157 took 7.855665922164917\n",
      "Epoch:  2158\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2158 took 7.848308324813843\n",
      "Epoch:  2159\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2159 took 7.850389003753662\n",
      "Epoch:  2160\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2160 took 7.8579535484313965\n",
      "Epoch:  2161\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2161 took 7.840331792831421\n",
      "Epoch:  2162\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2162 took 7.861364364624023\n",
      "Epoch:  2163\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2163 took 7.852160930633545\n",
      "Epoch:  2164\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2164 took 7.850990056991577\n",
      "Epoch:  2165\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2165 took 7.858716726303101\n",
      "Epoch:  2166\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2166 took 7.842636346817017\n",
      "Epoch:  2167\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2167 took 7.8575849533081055\n",
      "Epoch:  2168\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2168 took 7.84604811668396\n",
      "Epoch:  2169\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2169 took 7.850551128387451\n",
      "Epoch:  2170\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2170 took 7.846871614456177\n",
      "Epoch:  2171\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2171 took 7.857873201370239\n",
      "Epoch:  2172\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2172 took 7.849484205245972\n",
      "Epoch:  2173\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2173 took 7.853373050689697\n",
      "Epoch:  2174\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2174 took 7.850725889205933\n",
      "Epoch:  2175\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2175 took 7.857189893722534\n",
      "Epoch:  2176\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2176 took 7.849774599075317\n",
      "Epoch:  2177\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2177 took 7.8599748611450195\n",
      "Epoch:  2178\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2178 took 7.854369640350342\n",
      "Epoch:  2179\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2179 took 7.84192156791687\n",
      "Epoch:  2180\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2180 took 7.855488300323486\n",
      "Epoch:  2181\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 2181 took 7.864520072937012\n",
      "Epoch:  2182\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-32e9e6b353da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#generator_loss_val = generator_model.train_on_batch(np.random.uniform(-1,1,(BATCH_SIZE, INPUT_LEN)), positive_y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mgenerator_loss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINPUT_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mgenerator_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_loss_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "intervado_guardado = 50\n",
    "for epoch in range(initial_epoch, final_epoch):\n",
    "    start = time()\n",
    "    np.random.shuffle(images)\n",
    "    print(\"Epoch: \", epoch)\n",
    "    print(\"Number of batches: \", int(n_images // BATCH_SIZE))\n",
    "    discriminator_loss = []\n",
    "    generator_loss = []\n",
    "    minibatches_size = BATCH_SIZE * TRAINING_RATIO\n",
    "    print('Tenemos ', int(n_images // (BATCH_SIZE * TRAINING_RATIO)), ' minibatches.')\n",
    "    for i in range(int(n_images // (BATCH_SIZE * TRAINING_RATIO))):\n",
    "        discriminator_minibatches = images[i * minibatches_size: (i + 1) * minibatches_size]\n",
    "        \n",
    "        for j in range(TRAINING_RATIO):\n",
    "            image_batch = discriminator_minibatches[j * BATCH_SIZE: (j + 1) * BATCH_SIZE]\n",
    "            noise = np.random.normal(0, 1, (BATCH_SIZE, INPUT_LEN)).astype(np.float32)\n",
    "            #noise = np.random.uniform(-1,1,(BATCH_SIZE, INPUT_LEN)).astype(np.float32)\n",
    "            discriminator_loss_val = discriminator_model.train_on_batch([image_batch, noise], [positive_y, negative_y, dummy_y])\n",
    "            discriminator_loss.append(discriminator_loss_val)\n",
    "        \n",
    "        #generator_loss_val = generator_model.train_on_batch(np.random.uniform(-1,1,(BATCH_SIZE, INPUT_LEN)), positive_y)\n",
    "        generator_loss_val = generator_model.train_on_batch(np.random.normal(0, 1, (BATCH_SIZE, INPUT_LEN)), positive_y)\n",
    "        generator_loss.append(generator_loss_val)\n",
    "    \n",
    "    print(f'Epoch {epoch} took {time() - start}')    \n",
    "    \n",
    "    if epoch % intervado_guardado == 0:\n",
    "        print('Saving weights')\n",
    "        generator.save_weights(f'Weights/generator_epoch_{epoch}.h5')\n",
    "        discriminator.save_weights(f'Weights/discriminator_epoch_{epoch}.h5')\n",
    "        f.sample_best_images(generator, discriminator, output_dir, epoch, 10)\n",
    "        try:\n",
    "          os.remove(f'/content/drive/My Drive/app/Weights/discriminator_epoch_{epoch-intervado_guardado}.h5')\n",
    "        except:\n",
    "          pass\n",
    "        try:\n",
    "          os.remove(f'/content/drive/My Drive/app/Weights/generator_epoch_{epoch-intervado_guardado}.h5')\n",
    "        except:\n",
    "          pass\n",
    "        #f.generate_images(generator, output_dir, epoch, 10, method='FLAIR')\n",
    "        #f.generate_images(generator, output_dir, epoch, 10, method='T1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pKS_xfaGpe76"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "wgan.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
