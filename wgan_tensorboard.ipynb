{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25228,
     "status": "ok",
     "timestamp": 1558116562593,
     "user": {
      "displayName": "Gonzalo Izaguirre de Diego",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCpLYqbfMklo9sOPH3YrDMvbujoQ9DA9F42bYLdXg=s64",
      "userId": "16828157033214543773"
     },
     "user_tz": -120
    },
    "id": "5vCJONb-lLgx",
    "outputId": "4f931f49-99ef-494f-c59c-924030f9588d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 553,
     "status": "ok",
     "timestamp": 1558116609541,
     "user": {
      "displayName": "Gonzalo Izaguirre de Diego",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCpLYqbfMklo9sOPH3YrDMvbujoQ9DA9F42bYLdXg=s64",
      "userId": "16828157033214543773"
     },
     "user_tz": -120
    },
    "id": "67g850TRlpn3",
    "outputId": "7aa3b45d-a633-4b24-ce53-aaf0acc53008"
   },
   "source": [
    "import os \n",
    "os.getcwd()\n",
    "#path = os.join(os.getcwd(), '/drive/app')\n",
    "\n",
    "path = r'/content/drive/My Drive/app'\n",
    "os.chdir(path)\n",
    "print(os.getcwd())\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2276,
     "status": "ok",
     "timestamp": 1558088225011,
     "user": {
      "displayName": "Javier Gamazo Tejero",
      "photoUrl": "",
      "userId": "04168543225743776047"
     },
     "user_tz": -120
    },
    "id": "DwxpkQ1Gs515",
    "outputId": "9e6441b3-92cb-4f65-b9d1-261403be7412"
   },
   "outputs": [],
   "source": [
    "import funciones_wgan as f\n",
    "import numpy as np\n",
    "#import cv2\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, Conv2DTranspose\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from time import time\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZdIOCyyDkwXJ"
   },
   "source": [
    "# Variables generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NS5xllRdkwXK"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 9000\n",
    "BATCH_SIZE = 8\n",
    "# The training ratio is the number of discriminator updates\n",
    "# per generator update. The paper uses 5.\n",
    "TRAINING_RATIO = 1\n",
    "GRADIENT_PENALTY_WEIGHT = 10  # As per the paper\n",
    "INPUT_LEN = 128\n",
    "output_dir = 'output/'\n",
    "discriminator_weights = 'Weights/discriminator_epoch_950.h5'\n",
    "generator_weights = 'Weights/generator_epoch_950.h5'\n",
    "muestra = True # Si queremos coger una muestra de las imágenes. En true se cogen 140 imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(callback, names, logs, batch_no):\n",
    "    for name, value in zip(names, logs):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.simple_value = value\n",
    "        summary_value.tag = name\n",
    "        callback.writer.add_summary(summary, batch_no)\n",
    "        callback.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ceeoa4I2QETJ"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    initial_epoch = int(generator_weights.split('_')[2].split('.')[0]) + 1\n",
    "except:\n",
    "    initial_epoch = 0\n",
    "final_epoch = initial_epoch + EPOCHS - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9DP95mVWs52C"
   },
   "source": [
    "# Discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F_h5Fxwes52D"
   },
   "outputs": [],
   "source": [
    "def make_discriminator():\n",
    "    \"\"\"Creates a discriminator model that takes an image as input and outputs a single\n",
    "    value, representing whether the input is real or generated. Unlike normal GANs, the\n",
    "    output is not sigmoid and does not represent a probability! Instead, the output\n",
    "    should be as large and negative as possible for generated inputs and as large and\n",
    "    positive as possible for real inputs.\n",
    "    Note that the improved WGAN paper suggests that BatchNormalization should not be\n",
    "    used in the discriminator.\"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 5, padding='same', strides=[2, 2], input_shape=(256, 256, 3)))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Convolution2D(64, 5, kernel_initializer='he_normal', strides=[2, 2], padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Convolution2D(128, 5, kernel_initializer='he_normal', padding='same', strides=[2, 2]))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Convolution2D(256, 5, kernel_initializer='he_normal', padding='same', strides=[2, 2]))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Convolution2D(512, 5, kernel_initializer='he_normal', padding='same', strides=[2, 2]))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Convolution2D(1024, 5, kernel_initializer='he_normal', padding='same', strides=[2, 2]))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(1024 * 4 * 4, kernel_initializer='he_normal'))\n",
    "    #model.add(LeakyReLU())\n",
    "    model.add(Dense(1, kernel_initializer='he_normal'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tT79oszus52F"
   },
   "source": [
    "# Generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhzjPvqXs52G"
   },
   "outputs": [],
   "source": [
    "def make_generator():\n",
    "    \"\"\"Creates a generator model that takes a 128-dimensional noise vector as a \"seed\",\n",
    "    and outputs images of size 256x256x3.\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(4 * 4 * 2048, input_dim=INPUT_LEN))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Reshape((4, 4, 2048), input_shape=(4 * 4 * 2048,)))\n",
    "    bn_axis = -1\n",
    "\n",
    "    model.add(Conv2DTranspose(1024, 5, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization(axis=bn_axis))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Conv2DTranspose(512, 5, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization(axis=bn_axis))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Conv2DTranspose(256, 5, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization(axis=bn_axis))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Conv2DTranspose(128, 5, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization(axis=bn_axis))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Conv2DTranspose(64, 5, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization(axis=bn_axis))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Conv2DTranspose(3, 5, strides=2, padding='same', activation='tanh'))\n",
    "    # El output de esta última es 256x256x3\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yO9tAxixs52I"
   },
   "source": [
    "# Carga de datos\n",
    "\n",
    "El archivo total_three_datasets_sorted.npy es una matriz que ya tiene las tres capas:\n",
    "* 0: T1\n",
    "* 1: FLAIR\n",
    "* 2: Máscara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lAk6-T1Ms52J"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    images = np.load('total_three_datasets_sorted.npy')\n",
    "except:\n",
    "    images = np.load('images_three_datasets_sorted.npy')\n",
    "    masks = np.load('masks_three_datasets_sorted.npy')\n",
    "    \n",
    "    # Normalizado entre -1 y +1. Esto lo hace sobre toda la imagen, no sobre la capa T1 o FLAIR por separado.\n",
    "    # No tengo muy claro que sea correcto\n",
    "    images = [2.*(image - np.min(image))/np.ptp(image) - 1 for image in images]\n",
    "        \n",
    "    images = np.concatenate((images, masks), axis=3)\n",
    "    \n",
    "    # El generador toma imágenes 256x256x3. Como las tenemos 200x200, hay que redimensionarlas:\n",
    "    dim_final = (256, 256)\n",
    "    images = np.array([cv2.resize(image, dim_final, interpolation = cv2.INTER_AREA) for image in images])\n",
    "    \n",
    "    np.save('total_three_datasets_sorted.npy', images)\n",
    "    del masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15195,
     "status": "ok",
     "timestamp": 1558088238021,
     "user": {
      "displayName": "Javier Gamazo Tejero",
      "photoUrl": "",
      "userId": "04168543225743776047"
     },
     "user_tz": -120
    },
    "id": "g-3cz_OKs52L",
    "outputId": "e086e696-dddd-40d1-c509-7ce3fcc34a66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2780, 256, 256, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O4QMHxqis52P"
   },
   "source": [
    "Si se trabaja con Tensorflow está bien porque los canales están en la última dimensión del array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9Idd_Htx7vU"
   },
   "outputs": [],
   "source": [
    "if muestra:\n",
    "    images_shuff = images[:]\n",
    "    np.random.shuffle(images_shuff)\n",
    "    images_shuff = images_shuff[0:140,...]\n",
    "    images = images_shuff[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16091,
     "status": "ok",
     "timestamp": 1558088238937,
     "user": {
      "displayName": "Javier Gamazo Tejero",
      "photoUrl": "",
      "userId": "04168543225743776047"
     },
     "user_tz": -120
    },
    "id": "WxFWfW7xs52P",
    "outputId": "efa6e737-7fd3-4970-e746-c5dec6700929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.85898185 -1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.amin(images[34,...,0]), np.amin(images[34,...,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16085,
     "status": "ok",
     "timestamp": 1558088238938,
     "user": {
      "displayName": "Javier Gamazo Tejero",
      "photoUrl": "",
      "userId": "04168543225743776047"
     },
     "user_tz": -120
    },
    "id": "HgLnLY-wkwXc",
    "outputId": "504613ee-e272-4fd7-93ca-e2a4262aa746"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    }
   ],
   "source": [
    "n_images = images.shape[0]\n",
    "print(n_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NfpmgyW4kwXe"
   },
   "source": [
    "# Redes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ywbKy8DdkwXf"
   },
   "source": [
    "Inicialización de generador y discriminador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17751,
     "status": "ok",
     "timestamp": 1558088240613,
     "user": {
      "displayName": "Javier Gamazo Tejero",
      "photoUrl": "",
      "userId": "04168543225743776047"
     },
     "user_tz": -120
    },
    "id": "s-WR3_Xas52c",
    "outputId": "bc5f5159-f6bf-4319-afe8-5a927239b271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Gonzalo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "generator = make_generator()\n",
    "discriminator = make_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wpMFotL6kwXk"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    generator.load_weights(generator_weights)\n",
    "    discriminator.load_weights(discriminator_weights)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2DrfbXc-kwXm"
   },
   "source": [
    "### Generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GTOoa2ens52e"
   },
   "outputs": [],
   "source": [
    "# The generator_model is used when we want to train the generator layers.\n",
    "# As such, we ensure that the discriminator layers are not trainable.\n",
    "# Note that once we compile this model, updating .trainable will have no effect within\n",
    "# it. As such, it won't cause problems if we later set discriminator.trainable = True\n",
    "# for the discriminator_model, as long as we compile the generator_model first.\n",
    "for layer in discriminator.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "discriminator.trainable = False\n",
    "generator_input = Input(shape=(INPUT_LEN,))\n",
    "generator_layers = generator(generator_input)\n",
    "discriminator_layers_for_generator = discriminator(generator_layers)\n",
    "generator_model = Model(inputs=[generator_input], outputs=[discriminator_layers_for_generator])\n",
    "# We use the Adam paramaters from Gulrajani et al.\n",
    "generator_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9), loss=f.wasserstein_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMHdQ35WkwXr"
   },
   "source": [
    "### Discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "exsb00Ffs52g"
   },
   "outputs": [],
   "source": [
    "# Now that the generator_model is compiled, we can make the discriminator\n",
    "# layers trainable.\n",
    "for layer in discriminator.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "for layer in generator.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "discriminator.trainable = True\n",
    "generator.trainable = False\n",
    "\n",
    "# The discriminator_model is more complex. It takes both real image samples and random\n",
    "# noise seeds as input. The noise seed is run through the generator model to get\n",
    "# generated images. Both real and generated images are then run through the\n",
    "# discriminator. Although we could concatenate the real and generated images into a\n",
    "# single tensor, we don't (see model compilation for why).\n",
    "real_samples = Input(shape=images.shape[1:])\n",
    "generator_input_for_discriminator = Input(shape=(INPUT_LEN,))\n",
    "generated_samples_for_discriminator = generator(generator_input_for_discriminator)\n",
    "discriminator_output_from_generator = discriminator(generated_samples_for_discriminator)\n",
    "discriminator_output_from_real_samples = discriminator(real_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zSFtg_Qhs52k"
   },
   "outputs": [],
   "source": [
    "# We also need to generate weighted-averages of real and generated samples,\n",
    "# to use for the gradient norm penalty.\n",
    "averaged_samples = f.RandomWeightedAverage()([real_samples,\n",
    "                                            generated_samples_for_discriminator])\n",
    "# We then run these samples through the discriminator as well. Note that we never\n",
    "# really use the discriminator output for these samples - we're only running them to\n",
    "# get the gradient norm for the gradient penalty loss.\n",
    "averaged_samples_out = discriminator(averaged_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4TmGyJyhs52m"
   },
   "outputs": [],
   "source": [
    "# The gradient penalty loss function requires the input averaged samples to get\n",
    "# gradients. However, Keras loss functions can only have two arguments, y_true and\n",
    "# y_pred. We get around this by making a partial() of the function with the averaged\n",
    "# samples here.\n",
    "partial_gp_loss = f.partial(f.gradient_penalty_loss,\n",
    "                            averaged_samples=averaged_samples, gradient_penalty_weight=GRADIENT_PENALTY_WEIGHT)\n",
    "# Functions need names or Keras will throw an error\n",
    "partial_gp_loss.__name__ = 'gradient_penalty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8xqzj98js52p"
   },
   "outputs": [],
   "source": [
    "# Keras requires that inputs and outputs have the same number of samples. This is why\n",
    "# we didn't concatenate the real samples and generated samples before passing them to\n",
    "# the discriminator: If we had, it would create an output with 2 * BATCH_SIZE samples,\n",
    "# while the output of the \"averaged\" samples for gradient penalty\n",
    "# would have only BATCH_SIZE samples.\n",
    "\n",
    "# If we don't concatenate the real and generated samples, however, we get three\n",
    "# outputs: One of the generated samples, one of the real samples, and one of the\n",
    "# averaged samples, all of size BATCH_SIZE. This works neatly!\n",
    "discriminator_model = Model(inputs=[real_samples, generator_input_for_discriminator],\n",
    "                            outputs=[discriminator_output_from_real_samples, discriminator_output_from_generator, averaged_samples_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rgkg3-r4s52r"
   },
   "outputs": [],
   "source": [
    "# We use the Adam paramaters from Gulrajani et al. We use the Wasserstein loss for both\n",
    "# the real and generated samples, and the gradient penalty loss for the averaged samples\n",
    "discriminator_model.compile(optimizer=Adam(0.0001, beta_1=0.5, beta_2=0.9),\n",
    "                            loss=[f.wasserstein_loss, f.wasserstein_loss, partial_gp_loss])\n",
    "\n",
    "# We make three label vectors for training. positive_y is the label vector for real\n",
    "# samples, with value 1. negative_y is the label vector for generated samples, with\n",
    "# value -1. The dummy_y vector is passed to the gradient_penalty loss function and\n",
    "# is not used.\n",
    "positive_y = np.ones((BATCH_SIZE, 1), dtype=np.float32)\n",
    "negative_y = -positive_y\n",
    "dummy_y = np.zeros((BATCH_SIZE, 1), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = './logs'\n",
    "if not os.isdir(log_path):\n",
    "    os.mkdir(log_path)\n",
    "callback = TensorBoard(log_path)\n",
    "callback.set_model(generator_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(callback, names, logs, batch_no):\n",
    "    for name, value in zip(names, logs):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.simple_value = value\n",
    "        summary_value.tag = name\n",
    "        callback.writer.add_summary(summary, batch_no)\n",
    "        callback.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85462
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2330867,
     "status": "error",
     "timestamp": 1558098111416,
     "user": {
      "displayName": "Javier Gamazo Tejero",
      "photoUrl": "",
      "userId": "04168543225743776047"
     },
     "user_tz": -120
    },
    "id": "9c7EK-V-s52t",
    "outputId": "2b120ce0-6c9e-468f-b468-7dde90727b96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  951\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 951 took 143.32084202766418\n",
      "Epoch:  952\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 952 took 139.83846163749695\n",
      "Epoch:  953\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 953 took 140.57446932792664\n",
      "Epoch:  954\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 954 took 143.28716254234314\n",
      "Epoch:  955\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 955 took 147.33034348487854\n",
      "Epoch:  956\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 956 took 144.62658524513245\n",
      "Epoch:  957\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 957 took 147.58366703987122\n",
      "Epoch:  958\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 958 took 144.48497223854065\n",
      "Epoch:  959\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 959 took 142.63991618156433\n",
      "Epoch:  960\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 960 took 142.84534215927124\n",
      "Epoch:  961\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 961 took 142.9939513206482\n",
      "Epoch:  962\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 962 took 141.42659878730774\n",
      "Epoch:  963\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 963 took 139.02856588363647\n",
      "Epoch:  964\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 964 took 144.90383982658386\n",
      "Epoch:  965\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 965 took 144.9517183303833\n",
      "Epoch:  966\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 966 took 144.58021569252014\n",
      "Epoch:  967\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 967 took 142.37060856819153\n",
      "Epoch:  968\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 968 took 150.80706238746643\n",
      "Epoch:  969\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 969 took 145.26289987564087\n",
      "Epoch:  970\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 970 took 161.4805417060852\n",
      "Epoch:  971\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 971 took 145.5488305091858\n",
      "Epoch:  972\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 972 took 145.73115372657776\n",
      "Epoch:  973\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 973 took 142.50824189186096\n",
      "Epoch:  974\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 974 took 143.35096955299377\n",
      "Epoch:  975\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 975 took 143.653080701828\n",
      "Epoch:  976\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 976 took 139.29283690452576\n",
      "Epoch:  977\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 977 took 139.51324367523193\n",
      "Epoch:  978\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 978 took 145.13834810256958\n",
      "Epoch:  979\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 979 took 139.947092294693\n",
      "Epoch:  980\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 980 took 148.0361213684082\n",
      "Epoch:  981\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n",
      "Epoch 981 took 146.44254803657532\n",
      "Epoch:  982\n",
      "Number of batches:  17\n",
      "Tenemos  17  minibatches.\n"
     ]
    }
   ],
   "source": [
    "intervado_guardado = 50\n",
    "\n",
    "for epoch in range(initial_epoch, final_epoch):\n",
    "    start = time()\n",
    "    np.random.shuffle(images)\n",
    "    print(\"Epoch: \", epoch)\n",
    "    print(\"Number of batches: \", int(n_images // BATCH_SIZE))\n",
    "    discriminator_loss = []\n",
    "    generator_loss = []\n",
    "    minibatches_size = BATCH_SIZE * TRAINING_RATIO\n",
    "    print('Tenemos ', int(n_images // (BATCH_SIZE * TRAINING_RATIO)), ' minibatches.')\n",
    "    for i in range(int(n_images // (BATCH_SIZE * TRAINING_RATIO))):\n",
    "        discriminator_minibatches = images[i * minibatches_size: (i + 1) * minibatches_size]\n",
    "        \n",
    "        for j in range(TRAINING_RATIO):\n",
    "            image_batch = discriminator_minibatches[j * BATCH_SIZE: (j + 1) * BATCH_SIZE]\n",
    "            noise = np.random.normal(0, 1, (BATCH_SIZE, INPUT_LEN)).astype(np.float32)\n",
    "            #noise = np.random.uniform(-1,1,(BATCH_SIZE, INPUT_LEN)).astype(np.float32)\n",
    "            discriminator_loss_val = discriminator_model.train_on_batch([image_batch, noise], [positive_y, negative_y, dummy_y])\n",
    "            discriminator_loss.append(discriminator_loss_val)\n",
    "        \n",
    "        #generator_loss_val = generator_model.train_on_batch(np.random.uniform(-1,1,(BATCH_SIZE, INPUT_LEN)), positive_y)\n",
    "        generator_loss_val = generator_model.train_on_batch(np.random.normal(0, 1, (BATCH_SIZE, INPUT_LEN)), positive_y)\n",
    "        generator_loss.append(generator_loss_val)\n",
    "        \n",
    "    \n",
    "    print(f'Epoch {epoch} took {time() - start}')\n",
    "    write_log(callback, ['g_loss'], [generator_loss_val], epoch)\n",
    "    \n",
    "    if epoch % intervado_guardado == 0:\n",
    "        print('Saving weights')\n",
    "        generator.save_weights(f'Weights/generator_epoch_{epoch}.h5')\n",
    "        discriminator.save_weights(f'Weights/discriminator_epoch_{epoch}.h5')\n",
    "        f.sample_best_images(generator, discriminator, output_dir, epoch, 10)\n",
    "        try:\n",
    "            os.remove(f'/content/drive/My Drive/app/Weights/discriminator_epoch_{epoch-intervado_guardado}.h5')\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            os.remove(f'/content/drive/My Drive/app/Weights/generator_epoch_{epoch-intervado_guardado}.h5')\n",
    "        except:\n",
    "            pass\n",
    "        #f.generate_images(generator, output_dir, epoch, 10, method='FLAIR')\n",
    "        #f.generate_images(generator, output_dir, epoch, 10, method='T1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pKS_xfaGpe76"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "wgan.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
